[
["index.html", "Geog-364: Introduction to R Chapter 1 Lab 1 1.1 Introduction 1.2 Installing R and R-studio 1.3 Getting started 1.4 Lab challenge 1 1.5 R coding basics 1.6 Lab Challenge 2 1.7 Create a R-Markdown document 1.8 “Friendly text” 1.9 Lab Challenge 3 1.10 Online courses and resources", " Geog-364: Introduction to R Helen Greatrex 2020-11-09 Chapter 1 Lab 1 1.1 Introduction 1.1.1 Why is this class in R? There are many different types of software one can use to analyze spatial data. We’re going to focus on the R programming language because: It’s free and open source. It allows you to do traditional statistics, machine learning and data analysis. It’s a good introduction to programming, which is great for your resume. There are some very cool visualization tools including RMarkdown, Blogdown and Bookdown, where you can integrate R analysis into your papers, blogs and websites. For example, this lab book is written in Bookdown. RShiny for very cool interactive dashboards Leaflet and RGoogleMaps where you can use beautiful map making tools You will hopefully be exposed to all of these things throughout the course of the semester. 1.1.2 What are R and R-studio? R is a programming language commonly used by statisticians and scientists across the world. It can be used to analyse data, to visualise results and to develop new software. It is commonly used in spatial analysis because it is powerful and free, plus it allows you to combine spatial analysis with more traditional statistics. When I say R is a programming language, I mean that it is a collection of commands that you can type into the computer in order to analyse and visualise your data. Think of it literally like a language like Spanish or Hindi, so learning it means learning vocabulary and grammar to communicate with your computer. It also means it will get easier with experience and practice.. When you install R on your computer, you are essentially teaching the computer what those the language means along with some very basic Notepad-like software where you could enter the commands. Figure 1.1: The basic R screen More recently, R-studio has been designed as a piece of software to make it easier to programme in R. It’s what Microsoft Word is compared to notepad - lots more functionality and things to click. For example, you can easily see help files, run code, see your output and link to all the website/dashboard builders. In this course, we are going to use R-Studio as the main software, but you don’t need it if one day you only have access to R. Figure 1.2: The basic R-studio screen. Here is the R-Studio website where you can explore some of its features: https://rstudio.com/products/rstudio/features/ 1.1.3 What are R-Markdown &amp; R-Shiny? In recent years, people have developed new software in R to allow you to include your analysis in some pretty sophisticated outputs. RMarkdown is an document written in R, where you can combine code, results, equations and text (and photos/video etc). Essentially R-Markdown is a way to tell a story along with your data analysis, without having to copy and paste screen-shots of diagrams. This will be how you write up your lab reports. R-Markdown also goes beyond just interactive documents: RBookdown is a book written in RMarkdown. It is how I wrote this help document RBlogdown is a blog written in RMarkdown You can also make entire interactive websites in R-Markdown You can see examples and more details about R-markdown here: https://rmarkdown.rstudio.com/gallery.html RShiny allows you to make interactive applications and tools in R. Here are some examples: https://shiny.rstudio.com/gallery/ 1.2 Installing R and R-studio In this class, we would like you to download and install both R and R-Studio onto your own computers. If the labs are causing major problems with your computer or your computer hardware is struggling (or you have any other software issue), Talk to Dr Greatrex. We can definitely fix this and there are other options for “online R” that you can use. As these all have their own issues, getting it installed on your computer is likely easiest and the one I would like you to try first. In general, you can reach out to any of the teaching staff if you have any issue at all - we have likely see the errors hundreds of times before and we are happy to help. 1.2.1 I have R/R-studio installed already It’s great you have them installed! Many of the packages that we are using rely on the most up-to-date version of R and R-studio, so please update them to the latest version. To do this, follow these instructions: https://uvastatlab.github.io/phdplus/installR.html#updateR (If you know this will lead to issues on another project, talk to Dr Greatrex before you do anything). 1.2.2 Installing R/R-studio for the first time You need to install two new software programs, one called R and and called R-Shiny. You can click here for written instructions: https://psu.instructure.com/courses/2057289/pages/how-to-install-or-update-r-and-r-studio?module_item_id=30182767 Or follow these two installation tutorials in video format: To install R, follow this video: To install R-studio, follow this video: 1.3 Getting started 1.3.1 Open R-studio Figure 1.3: Icons of the R program and R-studio Open R-studio (NOT R!). You should be greeted by three panels: The interactive R console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help/Viewer (tabbed in lower right) If you click on the View/Panes/Pane Layout menu item, you can move these around. I tend to like the console to be top left and scripts to be top right, with the plots and environment on the bottom - but this is totally personal choice. Figure 1.4: The basic R-studio screen when you log on If you wish to learn more about what these windows do, have a look at this resource, from the Pirates Guide to R: https://bookdown.org/ndphillips/YaRrr/the-four-rstudio-windows.html If you have used R before, you might see that there are variables and plots etc already loaded. It is always good to clear these before you start a new analysis. To do this, click the little broom symbol in your environment tab. 1.3.2 Change a few settings R-studio wants to be helpful and will try to re-load exactly where you were in a project when you log back in. This can get really confusing, so we are going to turn this off. ON A MAC: Click on the R-studio menu button on the top left of the screen, then click Preferences. ON A PC: Click on Tools-&gt; Global Options -&gt; Preferences Now: UNCLICK “Restore most recently opened project at startup” UNCLICK “Restore .RData into workspace at startup” Set “Save workspace to .RData on” exit to Never Figure 1.5: Change these 3 options, then press Apply and OK 1.3.3 Create an R-project We are going to use R-projects to store our lab data in. An R project is a place to store all your commands, data and output in the same place. R will know that it is all linked. This is useful because say you have Lab-2 open, but want to re-open Lab-1 to check some code, clicking the “Lab 1 project file” will load a whole new version of R with everything you did for Lab 1. It is really important that you stay organised in this course, for example making sure you know where you put your data and your labs. You will return to them in your final project, so it pays to be organised now. To encourage this, choose a place that makes sense on your computer, create a folder called GEOG364.You can either do this outside R studio in your file explorer, or inside the “Files tab” in R studio. You should store all your 364 files inside this folder Now we will create our first project (also see the pic): open R-Studio and click New Project, then click “new directory” then “new project”. Name your project Lab 1, then browse to your newly created GEOG364 folder and press open. Press OK and your project is created. Figure 1.6: Instructions to create an R project Your output should look something like this: Figure 1.7: Your screen after running the project Equally, R should now be “looking” inside your Lab 1 folder, making it easier to find your data and output your results. Try typing this into the console (INCLUDING THE PARANTHESES/BRACKETS) and see if it prints out the location of Lab 1. getwd() If you’re having issues at this point or haven’t managed to get to this step, STOP! Ask Dr Greatrex, Saumya or Harman for help. 1.4 Lab challenge 1 Now you have opened your project, take a screenshot of your R-studio page. It should look like Figure 1.7, e.g. with at least R version 4.0.2, with the Lab 1 project and the project stored in your GEOG-364 folder. To take a screenshot on a mac, press Command-3. The sreenshot will appear on your desktop To take a screenshot on a PC, press Alt + PrtScn Rename the screenshot to your “username_Lab1_Fig1”(for example for me it will be hlg5155_Lab1_Fig1), then place it in your Lab 1 sub-folder inside GEOG-364. This folder was created when you made the project You will need this later, so don’t skip this step. 1.5 R coding basics So now we FINALLY get to do some R-coding. 1.5.1 Basic arithmatic Remember that the aim of R is to type in commands to get R to analyse data. The console (see Figure 1.4) is a space where you can type in those commands and it will directly print out the answer. You’re essentially talking to the computer. The little “&gt;” symbol in the console means that the computer is waiting for your command. The simplest thing we could do in R is do arithmetic. Click on the console and try typing the following. Press enter and R will calculate the answer. 1 + 100 ## [1] 101 It should look something like this: Figure 1.8: Starting to enter commands in the console. I choose the sum 1+1 When using R as a calculator, the order of operations is the same as you would have learned back in school, so use brackets to force a different order. For example, try typing in each of the the commands below (in grey) 3 + 5 * 2 ## [1] 13 will give a different result to (3 + 5) * 2 ## [1] 16 Really small or large numbers get a scientific notation: 2/10000 ## [1] 2e-04 The e is shorthand for “multiplied by \\(10^{X}\\)”. So 2e-4 is shorthand for \\(2\\times10^{-4}\\) What if I press Enter too soon? If you type in an incomplete command, R will wait for you to complete it. For example, if you type 1 + and press enter, R will know that you need to complete the command So it will move onto the next line but the &gt; will have changed into a +, which means its waiting for you to complete the command. If you want to cancel a command you can simply hit the “Esc” key and R studio will reset. Pressing escape isn’t only useful for killing incomplete commands: you can also use it to tell R to stop running code (for example if it’s taking much longer than you expect), or to get rid of the code you’re currently writing. 1.5.2 Comparing things We can also do comparisons in R. Here we are asking R whether 1 is equal to 1. # note two equals signs is read as &quot;is equal to&quot; 1 == 1 ## [1] TRUE # inequality (read as &quot;is not equal to&quot;) 1 != 2 ## [1] TRUE # less than 3 &lt; 2 ## [1] FALSE # less than or equal to 1 &lt;= 1 ## [1] TRUE # greater than 1 &gt; 0 ## [1] TRUE # greater than or equal to 1 &gt;= -9 ## [1] TRUE 1.5.3 Variables and assignment It’s great to be able to do maths easily on the screen, but really we want to be able to save our results, or load in data tables etc, so we can do more complex commands. In R, we can give things a name. This is called a variable. So then, instead of typing the whole command, we can simply type its name and R will recall the answer. The way we store data into a variable is using the assignment arrow &lt;-, which is made up of the left arrow and a dash. You can also use the equals sign, but it can cause complications later on. Try typing this command into the console. x &lt;- 1/40 Notice that pressing enter did not print a value onto your screen as it did earlier. Instead, we stored it for later in something called a variable, with the name ‘x’. So our variable x is now associated with the value 0.025, or 1/40. You can print a variable on screen by typing its name, no quotes, or by using the print command. Try printing out your variable. You can do this in two ways, either by typing its name, or by using the print command. x ## [1] 0.025 print(x) ## [1] 0.025 Look for the Environment tab in one of the panes of RStudio, and you will see that ‘x’ and its value have appeared. This ‘x’ variable can be used in place of a number in any calculation that expects a number. Try typing log(x) ## [1] -3.688879 Notice also that variables can be reassigned: x &lt;- 100 print(x) ## [1] 100 x used to contain the value 0.025 and and now it has the value 100. Note, the letter x isn’t special in any way, it’s just a variable name. You can replace it with any word you like as long as it contains no spaces and doesn’t begin with a number. Different people use different conventions for long variable names, these include periods.between.words.1 underscores_between_words camelCaseToSeparateWords What you use is up to you, but be consistent. Combining variables You can now use multiple variables together in more complex commands. For example, try these commands: #Take the variable x, add 1 then save it to a new variable called y y &lt;- x + 1 # print the multiple of 2yx onto the screen y ## [1] 101 Now you can see that there are two variables in your environment tab, x and y. Where y is the sum of the contents of x plus 1. **The way R works is that first it looks for the commands on the right of the arrow. It runs all of them, calculates the result, then saves that result with the name on the left of the arrow. The right hand side of the assignment (right of the &lt;-) can be any valid R command. The right hand side is fully evaluated before any assignment occurs.The variable only saves the result, not the command itself. You can even use this to change your original variable . Try typing the code below in a few times into the console and see what happens. A short cut to do this is to type the commands the first time, then use the up-arrow on your keyboard to cycle back through previous commands you have typed x &lt;- x + 1 #notice how RStudio updates its description of x on the top right tab x # print the contents of &quot;x&quot; onto the screen ## [1] 101 Our variables don’t have to be numbers. They could refer to tables of data, or a spatial map, or any other complex things. We will cover this more in future labs. 1.5.4 Functions The power of R lies in its many thousands of built in commands, or functions. In fact, we have already come across one - the print command. Some more examples include: plot(x=1:10,y=1:10) This will plot the numbers 1 to 10 against 1 to 10 x &lt;- nchar(\"hello\") This will count the number of letters in the word “hello” (e.g. 5), then save it as a variable called x Watch this short video to learn three important facts about functions: The file.choose() command will let you interactively select a file and print the address out onto the screen. Try each of these out in your console for the file.choose() command: # Typing this into the console will print out the underlying code file.choose # Typing it WITH parentheses will run the command file.choose() # Typing a ? in front will open the help file for that command ?file.choose The round brackets/parentheses after the name ( ), mean that it is a command. Sometimes we need to give the command some additional information. Anything we wish to tell the command should be included inside the inside the parentheses (separated by commas). The command will literally only know about the stuff inside the parentheses. sin(1) # trigonometry functions. Apply the sine function to the number 1. ## [1] 0.841471 log(10) # natural logarithm. Take the natural logarithm of the number 10. ## [1] 2.302585 This following command will plot the number 1 to 10 against the numbers 12 to 20, along with some axis labels. When you run this, the plot will show up in the plots tab. plot(1:10,11:20,col=&quot;dark blue&quot;, xlab=&quot;x values&quot;,ylab=&quot;GEOG-364 is the best&quot;) # plot the numbers 1 to 10 against the numbers 11 to 20 1.5.4.1 Command help As mentioned above, typing a ? before the name of a command or function will open the help page for that command in the “environment” box on your screen. As well as providing a detailed description of the command and how it works, scrolling to the bottom of the help page will usually show a worked example of the code which you can copy into the console. You can then copy these line by line into the console to understand what is happening. ## This provides the help page for file.choose. ?file.choose Figure 1.9: What you should see after typing ?file.choose 1.5.5 Packages There are now several million commands/functions available for you to use in R. To make sure your computer doesn’t get overwhelmed, it doesn’t load all of these at once. In fact many need to be downloaded from the internet. A close analogy is your phone. There are several million things you can do on your phone, which are collated into bundles called apps. To use the features in an app, you need to click on the icon of the app to load it. Equally you need to download many apps from the internet to start with. So we have R: The programming language itself Functions: Which are specific commands or actions written in the R language Packages: Commands are grouped into bundles called packages, which we need to load every time. This mini video and tutorial also explains things clearly. Click on the link, work thorugh it and we will install our own packages in the next lab. 1.5.6 Coding help Don’t worry about trying to remember every function in R. In this course, you will be provided with most of the functions you need, but you can also simply look up what you need on Google. This cartoon from XKCD is a pretty normal way to think about programming! One of the most important skills you can learn as a programmer is how to effectively google the information you need… Figure 1.10: https://xkcd.com/627/ 1.6 Lab Challenge 2 Now we are going to download some packages from the internet and install them. You must be connected to the internet to make this happen! In the console, type the following commands, or click the “Install” button in the packages tab (next to plots) and find the package name. If it asks if you want to install dependencies, say yes. install.packages(&quot;knitr&quot;) install.packages(&quot;rmarkdown&quot;) install.packages(&quot;tidyverse&quot;) You will see a load of red text appear in the console as it tells you its very detailed status of how it’s downloading and installing. Don’t panic! It might also take several minutes to do this, longer on a bad internet connection. When you have typed all three commands and waited until they have finished running (remember, when it is done, you should see the little “&gt;” symbol in the console waiting for your next command), we want to check if they have installed successfully onto your computer. To do this we are going to load them using the library command: library(&quot;knitr&quot;) library(&quot;rmarkdown&quot;) library(&quot;tidyverse&quot;) If you have managed to install them successfully, often nothing happens - this is great! It means it loaded the package without errors. Sometimes, it will tell you friendly messages. For example, this is what shows up when you install the tidyverse package. It is telling you the sub-packages that it downloaded and also that some commands, like filter - now have a different meaning. E.g. originally the filter command did one thing, but now the tidyverse package has made filter do something else. Figure 1.11: Tidyverse install messages To find out if what you are seeing is a friendly message or an error, run the command again. If you run it a second time, then nothing should happen. ! IMPORTANT If you see some red errors here after mulitple attempts running the commands, we will have to fix what your computer is doing together. If you see errors, then take a screenshot of the full page and talk to a TA or Dr Greatrex, or post on piazza under the Lab 1 tab. 1.7 Create a R-Markdown document 1.7.1 What is markdown? So far, we’ve been typing commands into the console, but these will all be lost once you close R. Equally, it’s hard to remember or reproduce the analysis you have done . So we will now move onto writing code commands that you can save and submit. There are several types of document that you can create and save in R-Studio. A basic script (the filetype is .r). This is simply just a blank document where you can save code commands (a script is shown in (Figure 1.1)). When you “run” the commands in the script, R simply copy/pastes the commands over to the console. An R-Notebook or Markdown document (the filetype is .Rmd). These are documents you can use to write a report with normal text/pictures, but and also include both your R code and output. You can turn these into reports, websites, blogs, presentations or applications. For example these instructions are created using a markdown document. In this course we are going to focus on the R-Notebook format. Watch this 1 min video covers markdown files and leads to a comprehensive guide on how to create them. Markdown and Notebook files are almost identical, so I will use the terms interchangeably for now. 1.7.2 Creating a notebook document Time to make your own. Go to the File menu on the top left, then click New File - Notebook. A file should appear on your screen - your first notebook script. Essentially, we have some space for text, some space for code, and a space at the top of the file where we can add information about themes/styles etc. Figure 1.12: You should see TWO new files appear in your lab 1 folder If you click on the little right triangle arrow at the top-right of the code chunk, you can run the plot command, and a picture will appear, running the code. Note, it no longer runs in the console. You can still copy things into the console, by clicking on the line you want to run and pressing Ctrl-Enter / command-Enter. Let’s try this. On line 11, delete plot(cars) and type 1+1. Now press the green arrow and the answer should appear directly under your code chunk. Now click at the end of the script, around line 20 and press the green insert button (top right in the script menu icons). Insert a new R code chunk. Inside, type 1+2 and run it. So you can see that you can have text/pictures/videos etc, and code chunks interspersed throughout it. Finally, click on the script then press the save button. Save it as GEOG364_Lab1_PSU.ID e.g. GEOG364_Lab1_hlg5155 You will see two files appear in your files tab, and your Lab_1 folder. Click on the LabScript1_abc9999.nb.html file then it will open in your file browser - you should see your code output and your formatted text. You have made your first R-html output - congrats! IMPORTANTLY - If you don’t “run” each code chunk by clicking the arrow, or clicking the run-all button in the top right, then that code won’t run in your final html output that we grade. If that is the case we can’t see if the code works and cannot award you the marks! SO - get in the habit of pressing “run-all” before you submit your final html file Figure 1.13: You should see TWO new files appear in your lab 1 folder 1.7.3 Text formatting (in the white area) If you want to make bold/italic/underline text formatting, or to create section headings in your text, you need a few more commands. Remember, this is for the text in your final report in the white area - only R commands go in each code chunk! You need to press enter twice to start a new paragraph. As another example, if you type *TEXT*, you will see TEXT On a new line if you type # this is a new section header, then R will interpret that as a new chapter or section header and change the font accordingly. Here is what this text looks like in my script: Figure 1.14: My writing The easiest way to learn these is to click the Help/Markdown Quick Reference Menu, which will bring up a cheat-sheet with all the formatting. Although this feels like a pain, it’s actually useful because it makes it very easy for R to change the formatting depending on what output format you need. Figure 1.15: Bringing up the formatting cheatsheet THERE IS ALSO A SPELL-CHECKER NEXT TO THE SAVE BUTTON - REMEMBER TO USE IT! 1.8 “Friendly text” Much of what you see on your screen when you open a notebook document is simply a friendly introduction to RStudio. So your notebook file is essentially a Word document with the ability to add in “mini R-consoles”, AKA your code chunks. Imagine every time you opened a new word document, it included some “friendly text” (“hi, this is a word document, this is how you do bold text, this is how you save”). This is great the first time you ever use Word, but kind of a pain after that. RStudio actually does this. Every time you open a notebook file, you actually do get this friendly text, explaining how to use a notebook file. Read it, make sure you understand what it is telling you, then delete all of it. So delete from line 6 to the end. The stuff left over is your YAML code which tells R how to create the final html file. DON’T TOUCH THAT. Figure 1.16: You should see TWO new files appear in your lab 1 folder 1.9 Lab Challenge 3 Your final challenge. Delete all the “friendly text” in your script - you should be just left with your YAML code and a blank file (see previous section) Create a new level-1 section header called “R-Basics”, then press enter a few times (or your header will not work). See the formatting help if you are not sure how. Figure 1.17: Remember to add a blank line BEFORE AND AFTER your headings Now in the space below your header, add an interesting fact or something you learned about R. Write part of your fact in bold text. Below the text, add a code chunk and include some code of your choice that runs without an error. AT A MINIMUM, YOUR CODE CHUNK SHOULD INCLUDE Applying a function to your data Assigning the result to a variable Using the print command to print that variable out A common question is “but what code should I write?”. Surprise us! If you’re really new to all of this, you could choose something very similar to the simple exercises above, or if you are more experienced, you could stretch yourself with something more fun and complex (last year, someone made a llama plot..). The thing we are testing is that you understand how to create a code chunk and some basic code features. Below that code chunk, create a new heading called “Project Location” Remember the screenshot you took in Lab challenge 1? We are going to add it in here. First, lets find the file. It should be easy as you should have put it in your Lab 1 folder. R doesn’t automatically know which folder to look in, so that file.choose() command can help us find it. IN THE CONSOLE, type file.choose() (including the empty parentheses). A window will pop up. Find your screenshot then press OK. You will see in the console the file address has been written down. Now, go back to your script. Create a new code chunk and inside type the following, but replace FILEADDRESS with the full address that just appeared on your console (INCLUDING THE QUOTES). Now click run and the chunk should load your screenshot inside your R-script. As described in Lab challenge 1, we are checking the version of R, that you have created the project and stored it in the right place. # Load the knitr library library(knitr) #Add your screenshot knitr::include_graphics(&quot;FILEADDRESS&quot;) Figure 1.18: How your code should look but with your own picture and address Pictures are a little weird in R, which is why we’re taking our time with this lab. If your picture does not show up in the final nb.html: Did you run the code chunk in the R-script so that it showed up just below the code chunk? (not in the plots tab) If not, run the code chunk then save again You can “knit” the file by clicking knit (near the save button), which will create a html rather than a nb.html. This should include the picture. You are 100% fine to submit the html instead of the nb.html If it STILL doesn’t work, click in the text oart (outside the code chunk), create a new line and try the command ![](FILEADDRESS), where FILEADDRESS is the output of file.choose() in the console CLICK RUN-ALL, then save your file. Go to your Lab 1 folder and click on the nb.html file. This should now be a professional html file of your work. If you have issues, try pressing the knit button and look at the html file (either are fine to submit) Go to Canvas, complete the Canvas course and submit. Congratulations on making it through a long document. The first time in a new programming language is always tough, so nice work for making it this far. 1.10 Online courses and resources If you want to practice the basics,then there are several places you can take free introductory tutorials - all of which I recommend. These include: https://bookdown.org/ndphillips/YaRrr/ The Pirate’s Guide to R. FREE linkedin learning for R (Penn State log-in required): https://www.linkedin.com/learning/learning-r-2/r-for-data-science?u=76811570 FREE (basic account) code academy - https://www.codecademy.com/courses/learn-r/lessons/introduction-to-r/ Data Camp: First chapter is free - https://www.datacamp.com/courses/free-introduction-to-r If you’re completely lost - it’s OK! This is pretty normal at this stage. Talk to the instructors and we can point you in the direction of resources to make you feel more comfortable using R. "],
["lab-2.html", "Chapter 2 Lab 2 2.1 Learning objectives 2.2 Lab set-up 2.3 Data.frames and summary statistics 2.4 Converting to spatial data 2.5 Assigning a map projection 2.6 Interactive Leaflet plots 2.7 Describe the Ghana data 2.8 Show me something new 2.9 Lab submission check", " Chapter 2 Lab 2 2.1 Learning objectives The aim of this lab is to start you out exploring spatial data and to do some basic analysis. By the end of this lab, you should be able to: Read-in and summarise table/spreadsheet data (data.frames) Select subsets of a data.frame using square brackets and the $ sign Plot specific columns of a data.frame against each other (E.g. long/lat) Understand the main types of spatial data format in R Be able to convert your data to either sp or sf format Load regional and country borders using rnaturalearth Plot sp data using the base package and interactively using Leaflet Critically analyse rain-gauge locations in R. Add a theme, authorship and table of contents to your markdown files. 2.2 Lab set-up 2.2.1 Create a Lab 2 Project R Projects are useful because you can easily keep your labs and future projects separate, without losing your code and data. For example,if you want to go back to an old Lab (say Lab 1), you can simply go into the Lab folder and click Lab1.RProj - this will open R and all your old code so you can carry on where you left off. So, to keep things tidy on your computer, lets create a new R-Project. First, make a new project called Lab 2 inside your GEOG364 folder. Hint: Go to the main menu-bar at the very top of the screen, then click: File | New Project | New Directory | New Project. Then call the project Lab 2 and make sure it’s created as a sub-directory of your GEOG-364 folder. Or follow the instructions in Section 1.3.3. Now, go to the File menu/New File/R Notebook to create a new blank lab script. Delete the “friendly text” - see section 1.8 and save it as GEOG364_Lab2_PSU.ID.Rmd e.g. GEOG364_Lab2_hlg5155.Rmd You should see something like the figure below, If you type getwd() into the console, it should return the location of the Lab 2 folder on your computer. Figure 2.1: What your screen should look like, but the address of getwd() should be the one on your computer. Note that pressing save on the script has automatically made your .nb.html file 2.2.2 Markdown adding a theme While your lab script is blank, lets focus on the YAML code at the top (lines 1-4). This makes your html output and there are many ways to customise it. Your YAML code currently looks like this --- title: &quot;R Notebook&quot; output: html_notebook --- Change the YAML code to look like this (but with your name). Note, if you copy/paste this and it doesn’t work, sometimes the quote marks copy weirdly from the internet - try deleting and retyping the quotes. --- title: &quot;Geog-364: Lab 2&quot; author: &quot;Helen Greatrex&quot; date: &quot;`r Sys.Date()`&quot; output: html_notebook: toc: true toc_float: yes number_sections: true theme: lumen --- The elements we just added are: The title The author Today’s date A floating table of contents and numbered sections (this won’t appear until you start typing section headings) The document is now in the lumen theme Now, save your file, or click “preview” at the top of the script. See if it works. If you click the knit button to make a html rather than a nb.html, then you will need to add a few extra lines for both output formats. So yours should look something like this. Figure 2.2: Where the point &amp; click package install button is There is a reasonable chance this won’t work first time around, as editing the YAML code can be a pain. It is very case and space sensitive. For example, the spaces at the start of some lines are important and are created using the TAB KEY, not the space bar. There is one TAB key before html_notebook (which is now on a new line). There are two TAB KEYS before toc, toc_float, number_sections and theme Don’t continue until you can make and view your html or nb.html file. If it doesn’t work, ask for help before moving on Challenge You can view themes for markdown documents here. https://www.datadreaming.org/post/r-markdown-theme-gallery/ Choose one that works for you and edit your YAML code to load it. 2.2.3 Installing common spatial packages Here, we are going to install some of the most common spatial analysis packages: rgdal, raster, sp, sf,leaflet,tmap and rnaturalearth. This can take some time, so I suggest getting the download running, then returning here to continue reading through the lab notes. As discussed in the previous lab, you can install these packages EITHER by [1] Copy/Pasting this command into the CONSOLE (if you put it in a code chunk, it will download and install every single time you run your script, which we don’t need): install.packages(c(&quot;rgdal&quot;, &quot;raster&quot;, &quot;sp&quot;,&quot;sf&quot;,&quot;rnaturalearth&quot;, &quot;leaflet&quot;,&quot;tmap&quot;,&quot;remotes&quot;)) remotes::install_github(&quot;AndySouth/rnaturalearthhires&quot;) OR [2] Going to the Packages menu (next to plots), clicking install, then choosing each package one by one (make sure install dependencies is clicked). Figure 2.3: Where the point &amp; click package install button is Once the packages have finished downloading and you can see the &gt; symbol again in the console, you can move on. In your script, make a new R-code chunk, Copy this code inside and run it. library(rgdal) library(sf) library(raster) library(knitr) library(sp) library(raster) library(leaflet) library(tmap) library(rnaturalearth) You should see similar messages to ones that come up here. Figure 2.4: Common friendly messages If instead of this, you see loads of red error code: Re-run the code chunk. Sometimes that clears it up Check you have spelt everything correctly Try to re-install the package, then re-run the code chunk If you are still seeing errors, ask for help. 2.2.4 Download data for this lab Now you need to download some data from Dr Greatrex’s github repository: A table of weather station locations in Ghana. Copy/Paste this code either into the console to download the files. ## Note, the structure of an R command. We have a function (a command), download.file(). ## R knows it is a function because it has parentheses after it. ## Inside the parentheses are the arguments that R needs to carry out the command ## (e.g. the address and the filename), separated by commas filelocation &lt;- &quot;https://raw.githubusercontent.com/hgreatrex/GEOG364_Labs/master/Lab_2/Lab02_GhanaMetStations.csv&quot; download.file(filelocation, destfile = &quot;Lab02_GhanaMetStations.csv&quot; , verbose=TRUE) The file should appear in your Lab 2 folder. If not, you probably didn’t create the project above, or your getwd() didn’t show your Lab 2 folder. 2.3 Data.frames and summary statistics 2.3.1 Reading in your first file In your script, make a new Section heading called “Data Input”. If you can’t remember how to do this, see Section 1.7.3, or pull up the Markdown Quick Reference Guide (see the Figure below) Figure 2.5: The incredibly useful Markdown quick reference The data you just downloaded, Lab02_GhanaMetStations.csv is essentially a basic spreadsheet called a .csv file (each number is separated by a comma). You can open this in Excel or notepad to have a look, but don’t save it there. First, we need to load, or read, this file into R. To do this, create a new code chunk and copy this code into it, including comments. # The read.csv command reads in a csv table of data into R. # We then save the table as a variable called Stations. Stations &lt;- read.csv(&quot;./Lab02_GhanaMetStations.csv&quot;,as.is=TRUE) If it doesn’t work, double check that the file actually is in your Lab 2 folder. If it does work, you should see that “Stations” has turned up in your environment area on your screen, probably on the top right. If you click on its name, you can have a look at the table, which can be a useful cross check to make sure that it all read into R successfully. Or, if you prefer you can print it into the console or into a code chunk, by typing its name into the console. In R, this type of data (a table) is called a data.frame. Figure 2.6: First look at the Stations data 2.3.2 Summarising data There are several functions that you can use to summarise data, especially data in a table. These include: str(TABLENAME) summary(TABLENAME) Where TABLENAME is the name of the data.frame variable you wish to explore. Challenge Create a code chunk and apply the functions above to Stations. Then in the text below the code chunk, use the results of your code to answer these questions: How many rows of data does the table have? What is the average Longitude of the stations? 2.3.3 Selecting columns and rows Sometimes, we do not want to analyse at the entire data.frame. Instead, we would like to only look at one (or more) columns or rows. There are several ways we can select data. The first is we can use the $ symbol to select its name. For example, to select the Latitude column, you can use the command # COLUMN NAMES ARE CASE SENSITIVE Stations$Latitude Secondly, we can use square brackets to numerically choose a column. This follows the format: TABLENAME[ROWS,COLUMNS] # This will select the 5th row and 7th column Stations[5,7] # This will select the 2nd row and ALL the columns Stations[2,] # This will select the 3rd column and ALL the rows Stations[,3] # We can combine our commands, this will print the 13th row of the Longitude column # (no comma as we&#39;re only looking at one column) Stations$Longitude[13] # The : symbol lets you choose a sequence of numbers e.g. 1:5 is 1 2 3 4 5 # So this prints out rows 11 to 15 Stations[11:15,] # The &quot;c&quot; command allows you to enter whatever numbers you like. # So this will print out rows 4,3,7 and all the columns Stations[c(4,3,7),] Challenge In a new code chunk, write and run the code to: Select Rows:4,6,2 and Columns:1 and 2 , then assign it to a variable called subset. Print values/rows 45:50 of the Station.Name column (you will just see 5 names as the answer) Apply the median() function to the Latitude column and print the output. E.g. what is the MEDIAN latitude in the Stations dataset? There is one Station.Name that has been duplicated (a common issue). Apply the table() function to the appropriate column to work out which station it is. Write the name of the station as a COMMENT inside the code chunk below your line of code. 2.3.4 Basic plot We can now make a basic plot of our data. Here we can use the plot command along with our new found knowledge to select columns. plot(Stations$Longitude,Stations$Latitude) This gives us the basics but it could be made more professional. Type ?par into the console, or search for par in help, and R will display a load of arguments that can improve the plot. For example, here, I have changed the asp (aspect ratio), pch (point shape) and col (point color) arguments. plot(Stations$Longitude,Stations$Latitude, pch=16, col=&quot;blue&quot;) Challenge Make a new code chunk and make a plot of the station locations. Your plot should include: Red points An aspect ratio of 1 (asp argument) Better x and y labels (xlab and ylab arguments) A point size of 0.5 (cex arguments) Although this plot is OK, it’s not great. The next section shows how to improve using the specific spatial packages available in R. 2.4 Converting to spatial data R is not clever. It’s a bit like working with a toddler. We might know that the words Longitude and Latitude have special spatial meaning for our Stations table, but R is not intelligent enough to work this out. So, several dedicated packages have been developed in R specifically for spatial data. There are four “mainstream” packages dedicated to dealing with both vector and raster spatial data: sp sf raster terra (not covered in this course but likely the future) Many package authors will write their commands assuming that your data is in a specific spatial format (e.g. sp). So understanding how to read in data into different formats is a useful skill. There are also some very specific packages which are slightly different. We will touch upon later in the semester, such as the ppp package for point pattern analysis, or ncdf4 for netcdf analysis. Others might link to google maps or ESRI products. 2.4.1 Making a data.frame “spatial” We are now going to focus on two very similar packages, sp and sf. To convert a data.frame to a SF object, we use the st_as_sf command: #-------------------------------------------------------------------------- # To convert to sf format # Here, &quot;Longitude&quot; and &quot;Latitude&quot; should be the # EXACT CASE-SENSITIVE COLUMN NAMES of your x and y columns in your table. # There is nothing special about the words longitude and latitude here #-------------------------------------------------------------------------- Stations_sf &lt;- st_as_sf(Stations, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;)) Stations_sf ## Simple feature collection with 598 features and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: -3.033 ymin: 4.783 xmax: 1.65 ymax: 11.0833 ## CRS: NA ## First 10 features: ## Station.Name geometry ## 1 23007DI POINT (-1.95 4.783) ## 2 23007DIX POINT (-1.95 4.7833) ## 3 23005PRI POINT (-2.133 4.8) ## 4 23002SES POINT (-1.917 4.85) ## 5 23001AXM POINT (-2.2333 4.8667) ## 6 23001AM POINT (-2.233 4.867) ## 7 23004AGO POINT (-1.967 4.883) ## 8 23005TAK POINT (-1.766 4.89) ## 9 23003TDI POINT (-1.774 4.894) ## 10 18025ABR POINT (-2.05 4.9) To convert a data.frame to a SP object (a spatialpointsdataframe), we simply tell it what the coordinates are: #-------------------------------------------------------------------------- # Or convert the original table to sp format #-------------------------------------------------------------------------- coordinates(Stations) &lt;- c(&quot;Longitude&quot;,&quot;Latitude&quot;) # Print out the result. You can see that R now sees it as a &quot;Spatial Points Data.frame&quot; Stations ## class : SpatialPointsDataFrame ## features : 598 ## extent : -3.033, 1.65, 4.783, 11.0833 (xmin, xmax, ymin, ymax) ## crs : NA ## variables : 1 ## names : Station.Name ## min values : 01000BAB ## max values : 23098ACC Let’s talk about the new information R is providing. We have class: You can have a spatial points dataframe, a spatial lines dataframe, or a spatial polygons dataframe. So you can see what type of vector data you have features: How many points are there (equivalent to the number of rows) extent: What is the furthest North/South/East/West that the data extends crs: The map projection. See the next section variables How many attributes of the data do we have? We just have one, Station.Name names: The names of your attributes (the other things you can plot) min values and max values Some information about the attributes, in our case the first and last station. You might find that you run the coordinates command once without error, but the second time around you see something like this: Figure 2.7: A common error This is because the coordinates command changes a TABLE into spatial data. But if you run it again, you are trying to convert spatial data into more spatial data and R gets confused. So you can completely ignore this error - it means everything is working fine. Challenge Make a new heading called “Spatial Analysis” Underneath that, create a new code chunk and use the code above to convert your Stations data to sp format. In the console, type plot(Stations). Your plot should now look very different. Above the code chunk, in the white text, explain what you just did. Figure 2.8: Plotting the data now it is sp format 2.4.2 Reading in vector line data (rnaturalearth) Let’s now also include some vector-line data on top of our points, but adding in some regional administrative boundaries. In later labs, we will learn how to read in vector data from a file, but this time we are going to use data that is already built into R. This is part of the rnaturalearth package, which links automatically with the “Natural Earth” dataset, found here: https://www.naturalearthdata.com/features/ For administrative border data, we can use the ne_countries or the ne_statescommands. For example, this would load the entire world borders and assign it to a variable called worldborder. # You can choose if you want the output to be sf or sp data worldborder &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sp&quot;) plot(worldborder) and this would load the regional boarders for the United Kingdom # You can choose if you want the output to be sf or sp data UK &lt;- ne_states(country=&quot;united kingdom&quot;,returnclass = &quot;sp&quot;) plot(UK) Challenge In your script, make a new section heading called “Vector data”. Now make a new code chunk and use the ne_states command to load regional borders for Ghana. Assign it to a variable called ghana. Above the code chunk, in the white text, explain what you just did. 2.5 Assigning a map projection Map projections and coordinate system are an essential part of spatial statistics. Also, say we want to overlay country boarders on our points. In that case, the coordinate systems of the shapefiles need to be identical. R makes use of the PROJ4 library which is a set of character strings that defines a coordinate system (a datum and/or map projection definition). The PROJ4 character string syntax consists of a list of parameters, each prefixed with the + character. For example, the familiar UTM grid system is defined by +proj=utm the specific zone number (e.g., 19) is defined by +zone=19 and the datum (e.g., NAD 1983) is defined by +datum=NAD83 Putting these parameters together, we would see something like this: \"+proj=utm +zone=19 +datum=NAD83 +no_defs +ellps=GRS80 +togrs80\" A list of a the PROJ4 map projection names and their parameters used in defining a coordinate system can be found here: https://proj.org/operations/projections/index.html. We will also discuss this in more detail in later labs. If you are interested in projections, for a comprehensive guide, use this resource: https://bookdown.org/robinlovelace/geocompr/reproj-geo-data.html 2.5.1 Checking a projection in the sp package In most cases, dealing with spatial data will involve changing the datum definition, map projection, or one of the map projection parameters (such as the central meridian or standard parallels). You can check the existing projection using the projection command: ## Here SPATIALDATA is the name of the object you want to examine projection(SPATIALDATA) Challenge In a new code chunk, print out the projection of Stations and also the projection of ghana. You should see that one is “NA” and one is a common long-lat projection: \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\". Above the code chunk, in the white text, explain what you just did. This can causes problems! If our map projections are different for different datasets, then they will not align with each other in any plots or analysis. So lets change the projection of Stations. 2.5.2 Assign/changing projections in the sp package To change a projection: IF THE PROJECTION IS NA: To set the first ever coordinate system for an sp object, you would use the the CRS command # Nothing should happen when you run the code chunk if there&#39;s no error # Here SPATIALDATA is the name of the object you want to assign a projection to # proj4string(SPATIALDATA) &lt;- CRS(&quot;your projection code&quot;) #e.g. to change the projection of Stations to UTM, the code would be as follows proj4string(Stations) &lt;- CRS(&quot;+proj=utm +zone=19 +datum=NAD83 +no_defs +ellps=GRS80 +togrs80&quot;) # THIS WILL NOT WORK IF YOUR DATA ALREADY HAS A PROJECTION If your data ALREADY HAS A PROJECTION: you can change it using the spTransform command # Nothing should happen when you run the code chunk if there&#39;s no error # Here SPATIALDATA is the name of the object you want to assign a projection to Stations &lt;- spTransform(Stations,&quot;+proj=longlat +datum=WGS84&quot;) This also means that if you re-run your chunk you might get a warning or an error. Because you set the original projection once using CRS, but a quirk of R means it will show an error if you try to set the projection of something that already has one assigned. So again, if you are running twice and see an error you can ignore it. Challenge In a new code chunk, assign the projection of your Stations data to be this CRS: \"+proj=longlat +datum=WGS84\" 2.5.3 Map projections in the sf package The same kind of transformations can also take place in sf formats. Unlike with RGDAL, you would use sf_transform() for sf formatted data. 2.6 Interactive Leaflet plots Let’s now see how our data looks plotted. Create a new sub-heading called “Data Plots”, then in a new code chunk enter the following code. You should see a basic plot that looks like this. If so, congrats! If not, you need to adjust your projections or something has happened. plot(Stations) plot(ghana,add=TRUE) The plot above is still pretty basic, so lets explore some of the big packages available to let you make “pretty plots” of spatial data. There are three big mapping functions in R: ggmap, tmap and leaflet. Today, we are going to make some interactive maps using Leaflet - an interactive mapping tool that goes far beyond R. Watch the video to get a sense of it. To make our first Leaflet plot, we will ignore the %&gt;% symbol you saw in the video. Essentially we make a leaflet map by adding layers. At the end, we print the map by typing its name. # leaflet makes maps in layers and only at the end, prints out the final map #LAYER1 mymap &lt;- leaflet(data = Stations) #LAYER2 mymap &lt;- addProviderTiles(mymap, providers$Stamen.Toner) #LAYER3 mymap &lt;- addCircleMarkers(mymap, label = ~as.character(Station.Name), radius=.3,opacity = 0.8) # R doesn&#39;t care if one command is on 2 lines.. #LAYER4 mymap &lt;- addPolygons(mymap, data=ghana,weight=2,col = &#39;red&#39;,fillOpacity = 0) #LAYER5 mymap In this case we have: LAYER 1: Load the Stations data into leaflet and assign it to a variable called mymap LAYER 2: Use addProviderTiles to add a custom map background onto mymap. You can find different examples here: http://leaflet-extras.github.io/leaflet-providers/preview/index.html LAYER 3: Add custom markers to mymap, each one labeled with the Station.Name (taken from the Station.Name column of the data.frame) LAYER 4: Add the country borders from the ghana data. These are actually polygons, so make sure the fillOpacity is 0 to only show the border. LAYER 4: Print the final map Challenge Copy the code above into your own code chunk. Then change the map background to be something more informative (your choice). 2.7 Describe the Ghana data Even though we are going to learn some advanced spatial techniques in GEOG-364, one of the most important ways to analyse spatial data is to look at it and explore its features. We often want to map rainfall across the world. Because rainfall is a field (there is a value everywhere), it would initially make sense to spread your rain-gauges out in a uniform grid. Yet, in Ghana at least, this is not what is happening. Examine the Ghana rain-gauge data in your leaflet plot. Try different map backgrounds (terrain, land-use, others..). Try to understand why the data is spatially distributed the way it is. Now in your text, explain: What type of data are we looking at? What map projection does it have? You describe the spatial structure of the data and any key features that it has You describe why you think the data looks this way spatially. Provide evidence for your answers (you can have multiple plots if you want) What features in the data would concern you if you were a data scientist? Can you see any likely errors? 2.8 Show me something new This section is going to be a feature of all of your labs in GEOG-364 and likely in GEOG-464. It is designed to challenge you and push your R programming skills forward. One of the reasons an “A” is set at 90% is so that you can skip this section and still get an A, but I encourage you to try it as often as possible. Create a new section header called “Show me something new”. Beneath this, you need to show some new code or some new aspect of Markdown that goes beyond the lab. It doesn’t have to be long. You should also explain in your text or comments what the code does and what you are hoping to achieve. So you get used to the concept, in GEOG-364 I will always try to give some suggestions or inspiration. If you are struggling, you can show me some minor additions to existing code for 2.5/5 marks. For example: Copying code from earlier in the lab to plot the Station data, then changing the point type (pch) to crosses, or making the colors transparent Adding a new YAML element (you will have to do this at the top and describe what you did here) Showing me you know how to do some new markdown format (tables, links), https://rmarkdown.rstudio.com/lesson-6.html Something you think of - be creative! For full marks you need to demonstrates the use of a function or package that was not specifically covered in the handout, lecture, or lab. For example: Plotting the station data / borders using something like ggmap or tmap Loading the RColorBrewer data and using that for your plots Loading a rnaturalearth raster file and plotting that (bonus if it’s over Ghana and you can plot your points on top) Loading a relevant shapefile and plotting it over the Ghana data. Do something with a completely new package - this can be fun, like library(emojifont) Or your idea (welcomed) You will lose 2 marks for not explaining in the text what you are trying to achieve and what the commands are that do so. It should be clear you understand your code. 2.9 Lab submission check MARKDOWN/CODE STYLE - 5 MARKS Your code and document is neat and easy to read. You have section headings, have clearly labeled your answers or described what you are trying to do in the code. YAML - 10 marks You have adjusted your YAML code to include a theme, author, data and table of contents. This should show up in your final nb.html or html file R-BASICS - 30 marks You correctly answered all the questions in Section 2.3 “Data Frames” SPATIAL BASICS - 20 marks You checked the projections of Stations and ghana and adjusted the map projection of the Station data to be long/lat Section 2.10. You also plotted the ghana and Station data using the standard plot function and Leaflet (where you changed the theme) GHANA DESCRIPTION - 20 marks You included a thoughtful, well organized, and articulated response to the question about the Ghana gauges SOMETHING NEW - 5 marks You made a minor change to an existing piece of code (2.5/5) OR You demonstrated the use of a function or concept that was not specifically covered in the handout, lecture, or lab (5/5) HTML FILE SUBMISSION - 5 marks RMD CODE SUBMISSION - 5 marks [100 marks total] THERE IS NO CANVAS QUIZ WITH THIS LAB "],
["lab-3.html", "Chapter 3 Lab 3 3.1 Learning objectives 3.2 Lab 3 Set-Up 3.3 Initial Data exploration 3.4 Lab-3 Challenge 1 3.5 Mean Centre and Standard Ellipse 3.6 Lab-3 Challenge 2 3.7 Global Density analysis 3.8 Lab-3 Challenge 3 3.9 Local intensity - quadrat analysis 3.10 Lab-3 Challenge 4 3.11 Issues with quadrat analysis 3.12 Lab-3 Challenge 5 3.13 Kernel Density Estimation (KDE) 3.14 Lab-3 Challenge 6 3.15 Automatically calculating an “optimal” bandwidth 3.16 Lab-3 Challenge 7 3.17 Lab-3 Show me something new 3.18 Lab-3 submission check", " Chapter 3 Lab 3 3.1 Learning objectives Welcome to Lab 3, where we are going to focus on Point Pattern Analysis. A point pattern dataset events or objects occurring in a defined study region. These events could represent anything with a specific measurable location, from greenhouses to stars. Objective: The goal of Lab 3 is to learn some introductory spatial statistical approaches for characterizing first-order properties of a point pattern. We will be focusing on quadrat analysis, kernal analysis and standard ellipses. Data-sets: You will be working with two separate point pattern datasets that are already built into R. The first dataset, called longleaf pine, is a marked point pattern containing locations and diameter measurements of longleaf pint trees in a forest in Georgia. All your examples here in this lab book will analyse this dataset. The second dataset is called bei and is also related to trees. You will be analysing this one and will find out more as you go Figure 3.1: The two point datasets we are exploring 3.2 Lab 3 Set-Up 3.2.1 Create your Lab 3 project file Open a new version of R-studio. Click the file menu, then new project in a new directory. Select your 364 directory, then name the project Lab 3. If you are stuck on this process, see the start of Lab 1 and Lab 2. You can check this has worked by looking on the file explorer on your computer, then selecting your GEOG364 folder and checking that a Lab 3 folder has appeared with your Lab3.Proj file inside it. 3.2.2 Install new packages There are two specialist packages called “spatstat” and “car” which focus on point pattern analysis. 3.2.2.1 Install the spatstat package To make sure you do not include the install.packages command in your .Rmd file, please install it by clicking the “install button” in the packages menu. Click on the install button, then start typing spatstat,select it and press install. If you are a Windows user, you will see a message at the beginning of the install process saying that you need to install R-Tools. IGNORE THIS - You probably don’t need to install R-Tools Figure 3.2: You can click the button and simply type the name if the package you want to install The install might take a few minutes. Once it has finished, test it has worked by typing library(spatstat) into the console. you should see some messages like this. Figure 3.3: The messages that appear when I load spatstat for the first time There is a lot of text, but unless you have a load of things that say ERROR, you can ignore it. (Don’t worry about the out of date message). One way to check is to run the code chunk twice as shown in the screenshot. If the second time the messages disappear then you don’t need to worry. IF THIS PACKAGE HASN’T LOADED OR INSTALLED CORRECTLY, TALK TO AN INSTRUCTOR. 3.2.2.2 Install the car package Now, repeat the process for the car package. This might take a while to download. You should see something like this. Figure 3.4: The messages that appear when I load car for the first time IF THIS PACKAGE HASN’T LOADED OR INSTALLED CORRECTLY, TALK TO AN INSTRUCTOR. 3.2.3 Create your NoteBook file Now, go to File/New File and create a new R-NOTEBOOK file. Delete the friendly text (everything from line 6 onward) Save your file as GEOG364_Lab3_PSU.ID.Rmd e.g. GEOG364_Lab3_hlg5155.Rmd Follow Section 2.2.2 to modify your YAML code (you can keep lumen as a theme or try your own) Add a new code chunk. Inside add the following code and run it. library(spatstat) library(sp) library(car) The reason we are adding this just after typing it into your console is that these library commands need to be run every time you open R. So we put them in a code chunk right at the top of your Note Book file. Now click the Preview Button and make sure everything looks correct. IF THERE ARE PROBLEMS, TALK TO AN INSTRUCTOR 3.3 Initial Data exploration 3.3.1 Loading the data In this lab, we are going to use two datasets that are already built into the spatstat package. All of the examples in this lab book are based on a dataset called longleaf You will be conducting your analysis on a dataset called bei The advantage of this is that we don’t have to worry about file formats or Map Projections for now. But first, we need to load them into R studio Below your code chunk, make a new heading called Initial Data Analysis. Remember to leave a blank white line before and after your headings. If you can’t remember how to do this, go to Help/Markdown Quick Reference (where Help is the menu right at the top of your computer screen) or see Lab 1 or 2. Now make a new code chunk. Inside add: # This loads the internal dataset from R. data(longleaf) data(bei) Run the code chunk and you should see the longleaf and bei datasets appear in your environment quadrant on the screen (top right). From now onward, you do not have to run the longleaf examples because you can see the output in this lab script, but the data is there if you want to test examples. Your job is to understand the bei dataset. 3.3.2 Reading more information Because the longleaf and bei datasets are internal, they have their own help file. To view the help file for the longleaf data, I create a new code chunk, and run the command: # putting a ? at the start means look at the help file # this will only work for commands and data built into R. ?longleaf The help file now opens onto my screen: Figure 3.5: The longleaf help file We can see a lot of information from this file: My data is a point pattern dataset containing locations and diameter measurements of longleaf pint trees in a US forest in Georgia There are 586 pine trees in the dataset The data was collected and analysed by Platt, Evans and Rathbun in 1988 The data is marked. So it is not just locations, but it also contains additional information about the diameter of each tree trunk (in cm). The data is stored as a “ppp” object (which is a point pattern file type) The ppp object also contains information on the region (i.e., window) that was chosen for the study. In this case it is a rectangle with an X coordinate range of 0-200 and a Y coordinate range of 0-200, with units in meters. I then made an initial plot of the longleaf data. Because the data is marked, the size of the circle automatically corresponds to the marks, the diameter of the tree trunk. plot(longleaf) 3.4 Lab-3 Challenge 1 Make sure you have a code chunk that loads the packages using library, AND one that loads the data using the data commnd. If not, go back and reread the section. In a new code chunk, use the instructions above to load the help file for the bei data. Understand what the data is, how much data there is and whether it is marked. You are welcome to write notes here, but you will be assessed on your knowledge of this in the Canvas lab-3 quiz Q1 and Q2. Create a new code chunk and make a plot of the bei data. Comment underneath the plot about whether you see any spatial autocorrelation (positive/negative/zero), plus which spatial fallacies might cause problems in this lab. 3.5 Mean Centre and Standard Ellipse Firstly, let’s look at some summary statistics. The easiest thing to consider is the spatial mean, where we take the mean of all the x values vs all the y values. x.mean.longleaf &lt;- mean(longleaf$x) y.mean.longleaf &lt;- mean(longleaf$y) print(x.mean.longleaf) ## [1] 98.73527 print(y.mean.longleaf) ## [1] 113.8293 We can also add a standard ellipse using the dataEllipse command in the car package. dataEllipse(longleaf$x, longleaf$y, levels=0.68, fill=TRUE, fill.alpha=0.1, # Fill the ellipse with transparent grey pch=16, cex=.5, # filled small dots xlab=&quot;x (m)&quot;,ylab=&quot;y (m)&quot;, # labels col= c(rgb(red=0, green=0, blue=0, alpha = 0.5),&quot;blue&quot;)) # Slightly transparent points &amp; blue ellipse The reason this is often useful is that we can see if the overall distribution changes over time e.g. imagine repeating your survey of longleaf pines in 10yrs time and seeing if the distribution has changed with climate change or urbanization. 3.6 Lab-3 Challenge 2 Make a new heading called Standard Ellipse Make a new code chunk and find the spatial mean of the bei data Create a new code chunk and plot the standard ellipse of the bei data. This corresponds with questions 3 and 4 in the Canvas Lab 3 quiz. 3.7 Global Density analysis Let’s move beyond summary statistics. In the rest of this Lab, we will study the variation of the observations’ density across a study area, essentially giving us clues about the non-uniformity of space For example, the distribution of trees might vary across a landscape from North to South due to climate conditions, or show a more complex spatial pattern based on underlying soil characteristics. These underlying factors could easily result in some areas having dense clusters of trees and other areas causing clearings. This phenomenon is called First order spatial variation. AKA the underlying geography is causing the patterns we see (rather than a each individual point influencing the other points around it) Figure 3.6: plot from The first way to look at this is to examine the overall density of points over the entire field, known as the global intensity. This is simply the average number of points per grid-cell. We say it’s “global” because we only calculate one number for the entire study area. Mathematically, the overall intensity of a point process can be calculated via the following equation, where n is the number of points within our region and A is the area of the region. \\[\\lambda=\\frac{n}{A}\\] In R, we can do this simply by looking at the summary command. For the longleaf data, I am going to make a new code chunk and run summary(longleaf) ## Marked planar point pattern: 584 points ## Average intensity 0.0146 points per square metre ## ## Coordinates are given to 1 decimal place ## i.e. rounded to the nearest multiple of 0.1 metres ## ## marks are numeric, of type &#39;double&#39; ## Summary: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 9.10 26.15 26.84 42.12 75.90 ## ## Window: rectangle = [0, 200] x [0, 200] metres ## Window area = 40000 square metres ## Unit of length: 1 metre Where we can see: That the global intensity of the pattern is 0.0146 trees per square metre (note the units). This also gives us a summary of some information from the help file It also summarizes our marks. We can see that the thinnest tree trunk has a diameter 2cm and the largest, 75.9cm. Finally it tells us the shape and area of our boundary box containing all our points is 200x200m. Note, this bounding box doesn’t have to be rectangular. 3.8 Lab-3 Challenge 3 Create a new subheading called global intensity Create a new code chunk and use the example above to work out the global intensity of trees in the bei dataset 3.9 Local intensity - quadrat analysis Instead of a single number for intensity, we can examine how the point density varies across the study area. The simplest way to do this is by a technique called quadrat analysis. To do this we will: Divide the region into a series of equally sized quadrats/rectangles Calculate the intensity of the point process in each quadrat, to see how it varies This can be done in R using the quadratcount command from the spatstat package. Q &lt;- quadratcount(longleaf, nx = 4, ny = 4) print(Q) ## x ## y [0,50) [50,100) [100,150) [150,200] ## [150,200] 34 49 25 36 ## [100,150) 34 48 78 46 ## [50,100) 43 26 37 47 ## [0,50) 33 18 15 15 The above command divides the longleaf dataset in 16 equally sized square grid-cells, or quadrats. I have specified 4 quadrats in the x direction and 4 in the y direction. It then returns a count of the number of points in each quadrat as well as the size of each quadrat. In this case we can see that the area is 50*50 = 2500m The above data is for counts, so we can either divide each value by 2500 or simply use the intensity command to print the intensity: # Print the density for each quadrat # This is literally the counts in each box divided by the area of each box, 2500m^2 intensity(Q) ## x ## y [0,50) [50,100) [100,150) [150,200] ## [150,200] 0.0136 0.0196 0.0100 0.0144 ## [100,150) 0.0136 0.0192 0.0312 0.0184 ## [50,100) 0.0172 0.0104 0.0148 0.0188 ## [0,50) 0.0132 0.0072 0.0060 0.0060 We can also plot this to make it easier to interpret: plot(longleaf, # I&#39;m plotting the longleaf data use.marks = F, # I didn&#39;t want it to plot the different circle sizes so i turned off marks cex = 0.5, pch = 4, # point size and shape main=&quot;Longleaf quadrat count&quot;) # plot title Q &lt;- quadratcount(longleaf, nx = 4, ny = 4) # Calculate the quadrat counts. plot(Q, add = TRUE, cex = 2) # Now I add Q on top. I can see the there are 15 points in the bottom right quadrat compared to 34 in the top left. In future labs, we will assess if this variation is statistically significant. Or we can also plot the intensity, because the colours make it simpler to see patterns. # Plot the intensity plot(intensity(Q, image=TRUE), main=&quot;Point intensity&quot;, las=1) # Plot intensity raster plot(longleaf, pch=20, cex=0.6, add=TRUE) # Add points Finally, I am going to calculate the Variance Mean Ratio, which gives an assessment about whether the data is clustered, uniform or random (AKA levels of spatial autocorrelation): # we use the as vector command to convert the data from a spatial ppp object back to a normal number longleaf.variance &lt;- var(as.vector(Q)) longleaf.mean &lt;- mean(as.vector(Q)) longleaf.VMR &lt;- longleaf.variance / longleaf.mean print(longleaf.VMR) ## [1] 6.918721 Overall, does this show the data is clustered or not? 3.10 Lab-3 Challenge 4 Create a new subheading called quadrat analysis Follow the example above, but replacing longleaf with bei - to create a quadrat analysis for the bei data. You should include: A table of counts A table of intensities A plot of counts A plot of intensities Calculate the Variance Mean Ratio for the bei data. You will need these to answer questions in the Lab 3 quiz. 3.11 Issues with quadrat analysis Quadrat analysis is susceptible to some major spatial fallacies and problems. As described in the mgimond e-text-book: “The choice of quadrat shape and size will influence the measure of local density and must be chosen with care. If very small quadrat sizes are used you risk having many quadrats with no points which may prove uninformative. If very large quadrat sizes are used, you risk missing subtle changes in spatial density distributions.” For example, changing the quadrat box size affects the strength of the north-west / south-east gradient we see here. Equally, there are clear clusters in the underlying data and analysis hasn’t really captured those at all. I want to explore the impact of changing quadrat size with the longleaf data. par(mfrow=c(row=1,col=3),mar=c(1,1,1,3)) # set up 3 subplots in a row # 2 x 2 gridcells Q.A &lt;- quadratcount(longleaf, nx = 2, ny = 2) # Split into 2x2 quadrats plot(intensity(Q.A, image=TRUE), main=&quot;&quot;, las=1) # Plot intensity raster plot(longleaf, pch=20, cex=0.6, col=rgb(0,0,0,.5), add=TRUE) # Add longleaf points plot(Q.A, add = TRUE, cex = 2,col=&quot;white&quot;,font=2) # Add numbers #### # 4 x 4 gridcells Q.B &lt;- quadratcount(longleaf, nx = 4, ny = 4) # Split into 4x4 quadrats plot(intensity(Q.B, image=TRUE), main=&quot;&quot;, las=1) # Plot intensity raster plot(longleaf, pch=20, cex=0.6, col=rgb(0,0,0,.5), add=TRUE ) # Add longleaf points plot(Q.B, add = TRUE, cex = 2,col=&quot;white&quot;,font=2) # Add numbers #### # 12 x 3 gridcells Q.C &lt;- quadratcount(longleaf, nx = 12, ny = 3) # Split into 12 x 3 quadrats plot(intensity(Q.C, image=TRUE), main=&quot;&quot;, las=1) # Plot intensity raster plot(longleaf, pch=20, cex=0.6, col=rgb(0,0,0,.5), add=TRUE) # Add longleaf points plot(Q.C, add = TRUE, cex = 1,col=&quot;white&quot;,font=2) # Add numbers One final important note for quadrat analysis, is that it is typically good to keep the numbers IN EACH QUADRAT to be at least 30. Otherwise you have an even higher likelihood of seeing patterns just by random chance. 3.12 Lab-3 Challenge 5 Create a new code chunk, then copy and paste the code above into it. Replace all the longleafs with bei (there are 6 instances) and run to check it works Now, modify the code chunk to: Make your subplots in 1 column but in 3 rows (e.g. arranged vertically) plot quadrat counts for 4 quadrats (2 in the x direction and 2 in the y direction), 20 quadrats (5 in the x direction and 4 in the y direction), 100 quadrats (10x10) Below the code chunk, write about the features you observe when modifying the number of quadrats for the bei data. This is a classic example of a spatial fallacy. Which one and why? 3.13 Kernel Density Estimation (KDE) Kernel Density Estimation is another method that we can use to explore point pattern densities. As described in https://sites.warnercnr.colostate.edu/wtinkham/courses-taught/nr-512-spatial-statistical-modeling/nr-512-exercise-descriptions/nr-512-exercise-3/, \"Quadrat counting is useful for characterizing the intensity of an inhomogeneous point process, however there are limitations including: – The choice of origin, quadrat orientation, and quadrat size affects the observed frequency distribution. – A significant amount of spatial data is lost. KDE algorithms use a moving window approach to characterize intensity. This approach tends to preserve a greater amount of spatial detail and does not suffer as much from choices of origin, quadrat orientation, and quadrat size as compared to quadrat counting.\" Conceptually, a smoothly curved surface is fitted over each point. The surface value is highest at the location of the point and diminishes with increasing distance from the point. There will be more about this in the lectures. To apply it in R, we simply use the density command. For example, for the longleaf data, I would create a new code chunk and type. PointDensity &lt;- density(longleaf) plot(PointDensity) # Individual events can be added to the plot by adding our original points to the plot. plot(longleaf, add = TRUE, use.marks = F, cex = 0.5,pch=16) # i&#39;m ignoring the tree diameters and want small filled dots So you can see that instead of quadrats, we now have a smooth surface. In the density command above, the size &amp; shape of the smoothing window, (known as the “bandwidth”) is calculated automatically using a ‘rule of thumb’. It defaults to 0.9 times the minimum of the standard deviation and the interquartile range divided by 1.34 times the sample size to the negative one-fifth power. However, the bandwidth can be scaled via the sigma parameter within the density command: par(mfrow=c(row=1,col=3),mar=c(1,1,1,1)) # set up 3 subplots in a row plot (density(longleaf, sigma = 5, # change bandwidth by altering the value assigned to sigma main=&quot;Bandwidth=5&quot;)) plot(longleaf, add = TRUE,use.marks = F, cex = 0.5,pch=16) plot (density(longleaf, sigma = 20, # change bandwidth by altering the value assigned to sigma main=&quot;Bandwidth=20&quot;)) plot(longleaf, add = TRUE,use.marks = F, cex = 0.5,pch=16) plot (density(longleaf, sigma = 100, # change bandwidth by altering the value assigned to sigma main=&quot;Bandwidth=100&quot;)) plot(longleaf, add = TRUE,use.marks = F, cex = 0.5,pch=16) 3.14 Lab-3 Challenge 6 Create a new sub-heading called Kernel Density Estimation Use the example above to run KDE for the bei data and try modifying the bandwidth using the following numbers: 5, 10, 30, 70, 100, 200. (you just have to edit the bandwidth and discuss the result. YOu do not have to show all 7) Below the code chunk, write what impact modifying the bandwidth has on your results. What happens at very low vs very high bandwidths? 4.What is the “best” bandwidth? Make sure your plot shows your final choice Why did you choose that number? 3.15 Automatically calculating an “optimal” bandwidth Different statisticians have come up with different ways to automatically calculate the “optimal” bandwidth. For example 1. bw.diggle() - minimizes the mean-square error criterion defined by Diggle (1985) 2. bw.frac() - uses a quantile of the distance between two independent random points in the window 3. 3. 3. bw.stoyan() - sets the standard deviation of the kernel You can look at the R help to understand in more detail how they work (e.g. type ?bw.diggle into your console) Here I will apply the diggle() function to estimate a bandwidth for the data # Report the value of the bandwidth optimal_diggle &lt;- bw.diggle(longleaf) # do the density plot plot (density(longleaf, sigma = optimal_diggle), # use the diggle function to automatically guess bandwidth main=paste(&quot;Diggle Bandwidth Estimation = &quot;,round(optimal_diggle,3))) plot(longleaf, add = TRUE,use.marks = F, cex = 0.5,pch=16) 3.16 Lab-3 Challenge 7 Apply the code above to your script to calculate and plot an “optimal” bandwidth for the bei data. Finally, reflect on the lab and look back at the outputs you have made. If someone asks you about the spatial distribution of trees in the area, what would you tell them and which plots and analyses would you use (or not use!) to provide evidence. Write your thoughts below the code chunk. 3.17 Lab-3 Show me something new Make a new sub-heading called Something new. You have three options this week: A - You can show me some minor additions to existing code for 2.5/5 marks. For example: Showing me you know how to do some new markdown format (tables, links - see here for inspiration: https://www.statpower.net/Content/310/R%20Stuff/SampleMarkdown.html) Adding a new minor YAML element (you will have to do this at the top and describe what you did here) Adding contours to the Kernel density data using a command such as contour(density(bei), add=TRUE, col = “white”) Create a 2x2 grid of plots or add a new plotting format. B - For 5/5 you need to demonstrates the use of a function or package that was not specifically covered in the handout, lecture, or lab. For example: Adding a new colour palette from somewhere like R colorbrewer to your Kernel Density plots Adding country or regional borders to these plots Add a paragraph where you explain quadrat and/or kernel density and properly include equations using the R markdown equation format. E.g. they should look like mathematical equations in a text book. Reading/loading in a new point pattern, converting to ppp and running one analysis on it Do something with a completely new package or function Delve into some of the more complexities of the analysis - https://mgimond.github.io/Spatial/point-pattern-analysis.html#local-density Apply one of the other optimal bandwidths to the KDE data e.g. bw.frac or bw.stoyan C - For 5/5, complete an optional extra extension to this work where we use quadrat analysis to assess whether elevation impacts the bei data. Go to this link: https://sites.warnercnr.colostate.edu/wtinkham/courses-taught/nr-512-spatial-statistical-modeling/nr-512-exercise-descriptions/nr-512-exercise-3/ Then scroll down to the end where it says: “In the quadrat counting example above, we used a series of equally sized quadrats to characterize the intensity of a ppp object. In quadrat counting methods, any choice of quadrats is permissible. From a theoretical viewpoint, the quadrats do not have to be rectangles of equal area, instead they could be regions of any shape. Quadrat counting is more useful if we choose the quadrats in a meaningful way. One way to do this is to define the quadrats using covariate information.” Work from that point onward to show how quadrat analysis can be used to link a variable such as elevation to your point pattern. 3.18 Lab-3 submission check HTML FILE SUBMISSION - 5 marks RMD CODE SUBMISSION - 5 marks LAB CHALLENGE 1 to 4 - 10 marks total You included all the requested code/analysis and your code runs for the BEI data (not the longleaf data!) LAB CHALLENGE 5 - 10 marks You thoughtfully explored the impact of changing the quadrat size and shape on the bei data, providing the requested code and outputs. LAB CHALLENGE 6 - 10 marks You chose your best bandwidth for the bei data, showed it on your plot and justified your choice. LAB CHALLENGE 7 - 10 marks You calculated an optimal bandwidth for the data and included a thoughtful, well organized, and articulated response to your reflections on the lab. SOMETHING NEW - 5 marks You made a minor change to an existing piece of code (2.5/5) OR You demonstrated the use of a function or concept that was not specifically covered in the handout, lecture, or lab (5/5) MARKDOWN/CODE STYLE - 5 MARKS Your code and document is neat and easy to read. You have section headings, have clearly labeled your answers or described what you are trying to do in the code. [60 marks total] THERE IS A CANVAS QUIZ WITH THIS LAB FOR THE OTHER 40 MARKS "],
["test-1.html", "Chapter 4 TEST 1 4.1 Set up the data 4.2 Lanternfly analysis 4.3 Beetle analysis", " Chapter 4 TEST 1 4.1 Set up the data library(spatstat) lanternfly &lt;- read.csv(&quot;./tmp1.csv&quot;) beetle &lt;- read.csv(&quot;./tmp2.csv&quot;) par(mfrow=c(1,2)) plot(lanternfly,pch=20,main=&quot;Lanternfly sightings&quot;,xlim=c(0,5),ylim=c(0,5)) plot(beetle,pch=20,main=&quot;Beetle sightings&quot;,xlim=c(0,5),ylim=c(0,5)) 4.2 Lanternfly analysis lanternfly.ppp &lt;- as.ppp(lanternfly,c(0,5,0,5)) # Count the number of points Q.counts &lt;- quadratcount(lanternfly.ppp,nx=5,ny=5) # we use the as vector command to convert the data from a spatial ppp object back to a normal number Q.variance &lt;- var(as.vector(Q.counts)) Q.mean &lt;- mean(as.vector(Q.counts)) Q.VMR &lt;- Q.variance / Q.mean print(paste(&quot;VMR: &quot; ,Q.VMR)) ## [1] &quot;VMR: 0.986111111111111&quot; plot(lanternfly.ppp, # I&#39;m plotting the longleaf data use.marks = F, # I didn&#39;t want it to plot the different circle sizes so i turned off marks cex = 1, pch = 4, # point size and shape main=&quot;Grid 1&quot;) # plot title plot(Q.counts, add = TRUE, cex = 2) # Now I add Q on top. plot(intensity(Q.counts, image=TRUE), main=&quot;Point intensity of lanternfly distribution&quot;, las=1) # Plot intensity raster plot(lanternfly.ppp, pch=20, cex=0.6, add=TRUE) # Add points Q.test.fly &lt;- quadrat.test(lanternfly.ppp, nx = 5, ny = 5) # Calculate the quadrat counts. ## Warning: Some expected counts are small; chi^2 approximation may be inaccurate Q.test.fly ## ## Chi-squared test of CSR using quadrat counts ## ## data: lanternfly.ppp ## X2 = 23.667, df = 24, p-value = 0.9616 ## alternative hypothesis: two.sided ## ## Quadrats: 5 by 5 grid of tiles 4.3 Beetle analysis beetle.ppp &lt;- as.ppp(beetle,c(0,5,0,5)) # Count the number of points Q.counts &lt;- quadratcount(beetle.ppp,nx=5,ny=5) # we use the as vector command to convert the data from a spatial ppp object back to a normal number Q.variance &lt;- var(as.vector(Q.counts)) Q.mean &lt;- mean(as.vector(Q.counts)) Q.VMR &lt;- Q.variance / Q.mean print(paste(&quot;VMR: &quot;, Q.VMR)) ## [1] &quot;VMR: 0.986111111111111&quot; plot(beetle.ppp, # I&#39;m plotting the longleaf data use.marks = F, # I didn&#39;t want it to plot the different circle sizes so i turned off marks cex = 1, pch = 4, # point size and shape main=&quot;Beetle distribution&quot;) # plot title plot(Q.counts, add = TRUE, cex = 2) # Now I add Q on top. plot(intensity(Q.counts, image=TRUE), main=&quot;Point intensity of beetle distribution&quot;, las=1) # Plot intensity raster plot(beetle.ppp, pch=20, cex=0.6, add=TRUE) # Add points Q.test.beetle &lt;- quadrat.test(beetle.ppp, nx = 5, ny = 5) # Calculate the quadrat counts. ## Warning: Some expected counts are small; chi^2 approximation may be inaccurate Q.test.beetle ## ## Chi-squared test of CSR using quadrat counts ## ## data: beetle.ppp ## X2 = 23.667, df = 24, p-value = 0.9616 ## alternative hypothesis: two.sided ## ## Quadrats: 5 by 5 grid of tiles "],
["lab-4.html", "Chapter 5 Lab 4 5.1 Lab 4 Set-Up 5.2 Challenge 1, Exploratory analysis 5.3 Challenge 2, set up the data 5.4 Challenge 3, Make data ppp 5.5 Challenge 4, quadrat analysis 5.6 Challenge 5, elevation links 5.7 Challenge 6 Nearest neighbour 5.8 Challenge 7 L Analysis 5.9 Challenge 8 Show me something new 5.10 Lab-4 submission check", " Chapter 5 Lab 4 Welcome to Lab 4, where we are going to continue to focus on Point Pattern Analysis. Specifically, we are going to move onto distance based approaches. Objective: The goal of Lab 4 is to learn some introductory spatial statistical approaches for characterizing second-order properties of a point pattern. We will be focusing on nearest neighbour analysis and the F, G, K and L functions. Data-sets: You will be conducting your analysis on a dataset on fossils in Pennsylvania. The data comes from a Paleobiology database that is maintained by an international non-governmental group of paleontologists. Members of the Paleobiology database add fossil occurrence data into the database from scientific publications If you wish to know more about the fossil data, you can see and explore it here: https://paleobiodb.org/#/ 5.1 Lab 4 Set-Up 5.1.1 Create your Lab 4 project file Open a new version of R-studio. Click the file menu, then new project in a new directory. Select your 364 directory, then name the project Lab 4. If you are stuck on this process, see the start of previous labs You can check this has worked by looking on the file explorer on your computer, then selecting your GEOG364 folder and checking that a Lab 4 folder has appeared with your Lab4.Proj file inside it. 5.1.2 Create your NoteBook file Now, go to File/New File and create a new R-NOTEBOOK file. Delete the friendly text (everything from line 6 onward) Save your file as GEOG364_Lab4_PSU.ID.Rmd e.g. GEOG364_Lab4_hlg5155.Rmd Follow Section 2.2.2 to modify your YAML code (you can keep lumen as a theme or try your own) 5.1.3 Download and run packages Follow the instructions in Section 3.2.2. to download and install the following packages USAboundaries elevatr Now add a new code chunk in your script. Inside add the following code and run it. library(maptools) library(spatstat) library(sp) library(raster) library(USAboundaries) library(sf) library(elevatr) The reason we are adding this just after typing it into your console is that these library commands need to be run every time you open R. So we put them in a code chunk right at the top of your Note Book file. Now click the Preview Button and make sure everything looks correct. IF THERE ARE PROBLEMS, TALK TO AN INSTRUCTOR Don’t continue until you can make and view your html or nb.html file. If it doesn’t work, ask for help before moving on 5.2 Challenge 1, Exploratory analysis Have you read and followed the lab set up above? (esp the code chunk) If not, go back and do that! You should also be inside your Lab 4 R project The data you will be analyzing comes from here https://paleobiodb.org. I downloaded every record they had for Pennsylvania. Look around the website (including. the FAQ). Make a new sub heading in your script called Data Description Explain what the data is showing and where it comes from For full marks for this section, include: A bullet pointed list A screenshot of something interesting with a caption For including captions, see section 9.5.1 here: https://rmd4sci.njtierney.com/figures-tables-captions-.html#figures. Remember you also have the markdown quick reference guide by going to the help menu at the very top of your screen, then Markdown quick reference. 5.3 Challenge 2, set up the data 5.3.1 A. Read in and explore the point data I have provided the code to read in your data. Create a new code chunk and copy the code below into your R-script. Replace the “WRITE YOUR COMMENTS HERE..” with your description of what is happening in the code #------------------------------------------------------------------------------------------------ # WRITE YOUR COMMENTS HERE TO DESCRIBE WHAT IS HAPPENING #------------------------------------------------------------------------------------------------ filelocation &lt;- &quot;https://raw.githubusercontent.com/hgreatrex/GEOG364_Labs/master/Lab_4/PA_fossils.csv&quot; download.file(filelocation, destfile = &quot;PA_fossils.csv&quot; , verbose=TRUE) mydata &lt;- read.csv(&quot;PA_fossils.csv&quot;) Create a new code chunk and use the dim, names and summary commands to work out how many rows and columns the data has. Use the table command on the period column (or your own method) to see how many fossils were found in the Mississippian time-period. 5.3.2 B. Read in the other data and set projections I have provided the code to read in your data. Create a new code chunk and copy the code below into your R-script. #------------------------------------------------------------------------------------------------ # WRITE YOUR COMMENTS HERE TO DESCRIBE WHAT IS HAPPENING # us_states is from the USAboundaries package #------------------------------------------------------------------------------------------------ state.border &lt;- us_states(states = &quot;pennsylvania&quot;) state.border &lt;- st_transform(state.border,6346) #------------------------------------------------------------------------------------------------ # WRITE YOUR COMMENTS HERE TO DESCRIBE WHAT IS HAPPENING # get_elev_raster is from the elevatr package #------------------------------------------------------------------------------------------------ elevation &lt;- get_elev_raster(state.border, z = 5,clip=&quot;locations&quot;) #------------------------------------------------------------------------------------------------ # WRITE YOUR COMMENTS HERE TO DESCRIBE WHAT IS HAPPENING #------------------------------------------------------------------------------------------------ mydata_sf &lt;- st_as_sf(mydata, coords = c(&quot;Long&quot;, &quot;Lat&quot;),crs=4326) mydata_sf &lt;- st_transform(mydata_sf,6346) As before, your job is to describe what is happening in each section of the code. Replace the “WRITE YOUR COMMENTS HERE..” with your description of each section of code. Remember if you are not sure what a command does: you can google it you can look at its helpfile by typing a ? before its name into the console e.g.?st_as_sf or you might have seen it in previous labs. Now, in a new code chunk, use the projection command to check the map projection of elevation, mydata_sf and state.border. They should be identical. Below the code chunk, explain why I chose this map projection for the data. You can use this website to find out more about its projection by searching for its “EPSG code” https://epsg.io/ (the code I chose for this projection is 6346). 5.4 Challenge 3, Make data ppp Similar to last week, we are now going to analyse the data using commands from the spatstat package. This package of commands we use for point pattern data requires that we convert the data into a data-format called ppp. To create a ppp object we need to use the as.ppp() command in R. Look at the help file for the ppp command by typing ?ppp into the console or searching the help tab for ppp. By examining the help file we see that the ppp command requires 4 types of information: - x coordinates of each event and y coordinates of each event (or a “sp” data format) - window size (i.e., region size), - any marks (i.e., values/data for each event). #------------------------------------------------------------------------------------------------ # Create the boundary for the ppp data (the state border), then create the ppp file #------------------------------------------------------------------------------------------------ # This just converts stateowin to the correct file format. stateowin &lt;- as(as_Spatial(state.border), &quot;owin&quot;) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded ## datum Unknown based on GRS80 ellipsoid in CRS definition # This reads the data into a ppp format, then sets the window my.ppp &lt;- as.ppp(mydata_sf) ## Warning in as.ppp.sf(mydata_sf): only first attribute column is used for marks Window(my.ppp) &lt;- stateowin #and then we plot plot(elevation) plot(my.ppp,use.marks = F,cex = 1, pch = 4,add=TRUE) Copy the code above into a new code chunk and make sure it runs. You might get errors about Discarded datum Unknown based on GRS80 ellipsoid in CRS definition, or only first attribute column is used for marks. Ignore them. Look at the output of the plot, which shows the points plotted over elevation data. Below the code chunk, write your assessment of the spatial structure of the fossils. In your opinion are they clustered/random or uniform? 5.5 Challenge 4, quadrat analysis Create a new heading called Quadrat Analysis. Use the code in Lab 3 and Test 1, to conduct a quadrat analysis on the data. You should include: The Variance Mean Ratio (and interpretation of the result) A plot of the quadrat counts. A hypothesis test (written up in English, but using the output of quadrat.test) A plot of the density of the points You can choose a number of grid squares that seem reasonable (enough to show variation, not so many that there are a load of zeros) 5.6 Challenge 5, elevation links Create a new heading called Elevation and below this, copy across the two code chunks below. # Set up 3 plots in a column par(mfrow = c(1,2)) #------------------------------------------------------------------------------------------------ # First plot the density #------------------------------------------------------------------------------------------------ plot(density(my.ppp),main=NULL) plot(my.ppp,cex = .5, pch = 4,add=TRUE) #------------------------------------------------------------------------------------------------ # Then plot the elevation #------------------------------------------------------------------------------------------------ plot(elevation) #------------------------------------------------------------------------------------------------ # and see if elevation is linked to the point density #------------------------------------------------------------------------------------------------ # convert the elevation to a ppp image elev.img &lt;- as.im.RasterLayer(elevation) #and look at the relationship with the points plot(rhohat(unmark(my.ppp), elev.img)) par(mfrow = c(1, 1)) From this output, write what appears to be the relationships between elevation and fossils. Do you think there might be a physical reason for this? When you look at the point data, do you see any other things you would like to compare the data to? 5.7 Challenge 6 Nearest neighbour Create a new heading called nearest neighbour (spelled either the American or British way!) The nndist command calculates the distance of the nearest neighbour for every point. Create a code chunk and run the nndist command on your ppp point pattern data. Assign the answer to a variable called myNNDist. Now, make a histogram of the NNDist output using the command hist(myNNDist,br=50) Underneath the code, explain what the output of the code is showing you. What units are the distances in? (hint, think what you learned about your map projection) We can now easily calculate the average nearest neighbor distance and the ratio. Note, the command for the ratio is named after the people who invented it, Clarke &amp; Evans. Type ?clarkevans into the help file for more details. Create a code chunk and add/run the following #------------------------------------------------------------------------------------------------ # Mean nearest neighbour #------------------------------------------------------------------------------------------------ print(&quot;The mean nearest neighbour distance is:&quot;) print(mean(round(myNNDist,3))) #------------------------------------------------------------------------------------------------ # R ratio #------------------------------------------------------------------------------------------------ R.Ratio &lt;- clarkevans(my.ppp) print(&quot;The R ratio is:&quot;) print(R.Ratio) The output of the R.Ratio likely gives you two values, naive and cdf. CDF is simply a mathematical way of taking into account edge effects. Below the code chunk output, write a sentence or two to describe why edge effects might be an issue in this case. Then write your interpretion of the output. Does this show the data is clustered, random or dispersed? What would R typically be for very disperse data? What would R typically be for very clustered data? 5.8 Challenge 7 L Analysis We will now conduct an L-Function analysis on the data. Make a new heading called L-function, Below, make a new code chunk and copy over this code and run it #------------------------------------------------------------------------------------------------ # Calculate the L function and its uncertainty envelope #------------------------------------------------------------------------------------------------ L.DATA &lt;- Lest(my.ppp) #------------------------------------------------------------------------------------------------ # Rather than simply saying something is “clustered” or “uniform” # depending on how it subjectively looks to the # IRP line, we can instead use Monte Carlo simulation to assess our data # against many L patterns that were caused by an Independent Random Pattern. # Here we generate 500 CSRs and calculate the L function for each one #------------------------------------------------------------------------------------------------ L_DATA_envelope &lt;- envelope(my.ppp, Lest, correction = &quot;Ripley&quot;, verbose = F, nsim=500, nrank=1, savepatterns = TRUE, savefuns = TRUE) #------------------------------------------------------------------------------------------------ # Plot the raw L-function data for different edge effects. Ignore any warnings #------------------------------------------------------------------------------------------------ plot(L.DATA, . - r ~ r, ylim=c(0-(max(L.DATA$iso-L.DATA$r)),max(L.DATA-L.DATA$r))) # Add the uncertainty envelope. plot(L_DATA_envelope, . - r ~ r,add=TRUE) Under the code chunk output, explain what is happening in the plot. What does the gray area represent? How many simulations were used to create the plot. What does it mean if the observed black line falls in the gray area? What does it mean if it’s above/below the gray area? 5.9 Challenge 8 Show me something new Here you have a choice. I have just signed the entire class onto data camp, so you now have access to all the courses. If you feel that you are struggling with R and need to get a better grasp of the basics, if you complete at least 3 chapters of the “Introduction to R” data camp course and include completion screen-shots in your lab script, you can have the 5 marks. (https://www.datacamp.com/courses/free-introduction-to-r, but you will need to sign up using the link I sent) OR, you can continue to do the classic “something new”. Remember you actually have to do something new, not repeat what you did in previous weeks You could look at some other point pattern tutorials because they are all linked into spatstat. There are loads of things you could do to build your knowledge of point pattern analysis http://spatstat.org/Melb2018/solutions/solution03.html (density) http://spatstat.org/Melb2018/solutions/solution04.html (poisson) http://spatstat.org/Melb2018/solutions/solution05.html (marked) http://spatstat.org/Melb2018/solutions/solution06.html (K and L functions) https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html You could try downloading and reading in a shapefile for PA and plotting it over your points. You could try making some cool plots using tutorials like this: Cool mapping tutorial: https://bhaskarvk.github.io/user2017.geodataviz/notebooks/02-Static-Maps.nb.html There are lots of ideas in the previous lab suggestions. 5.10 Lab-4 submission check HTML FILE SUBMISSION - 10 marks RMD CODE SUBMISSION - 10 marks WORKING CODE - 10 marks: Your code works and the output of each code chunk is included in the html file output (e.g. you pressed run-all before you finished) EASY TO READ LAB SCRIPT - 10 marks: Your final html file is neat and easy to read. For example, your headings are working, you have included any relevant formatting, new paragraphs are on new lines etc. CHALLENGE 1 - 5 marks: You thoughtfully explored and described the paleobiology data in Lab Challenge 1 CHALLENGE 2 &amp; 3-10 marks: You thoughtfully and correctly answered all the questions and read your data into Rstudio CHALLENGE 4-20 marks: You correctly completed your quadrat analysis with all the required components of the quadrat analysis CHALLENGE 5-10 marks: You correctly completed your elevation analysis with all the required components CHALLENGE 6&amp;7 10 marks: You correctly completed your nearest neighbour/L analysis with all the required components SOMETHING NEW - 5 marks You made a minor change to an existing piece of code (2.5/5) OR You demonstrated the use of a function or concept that was not specifically covered in the handout, lecture, or lab, OR completed 3/5 chapters of a data camp course (5/5) [100 marks total] "],
["lab-5.html", "Chapter 6 Lab 5 6.1 Lab 5 Set-Up 6.2 Challenge 1: Explore the data 6.3 Challenge 2: Exploratory analysis 6.4 Challenge 3: Thiessen polygons by hand 6.5 Challenge 4: Thiessen polygons in R 6.6 Challenge 5: Inverse Distance Weighting info 6.7 Challenge 6: Inverse Distance Weighting code 6.8 Challenge 7: Checking accuracy 6.9 Challenge 8: Your own IDW 6.10 Challenge 9 Show me something new 6.11 Lab-5 submission check", " Chapter 6 Lab 5 Welcome to Lab 5, where we are going to focus on interpolation techniques. Objective: The aim of this lab is to look at different ways of interpolating data, specifically Thiessen polygons and inverse distance weighting. Data-sets: You will be conducting your analysis on some environmental data in California. 6.1 Lab 5 Set-Up 6.1.1 Create your Lab 5 project file Open a new version of R-studio. Click the file menu, then new project in a new directory. Select your 364 directory, then name the project Lab 5. If you are stuck on this process, see the start of previous labs. You can check this has worked by looking on the file explorer on your computer, then selecting your GEOG364 folder and checking that a Lab 5 folder has appeared with your Lab4.Proj file inside it. 6.1.2 Create your NoteBook file Here you have a choice: Either.. you can create a standard lab script as before: Go to File/New File and create a new R-NOTEBOOK file. Delete the friendly text (everything from line 6 onward) Save your file as GEOG364_Lab5_PSU.ID.Rmd e.g. GEOG364_Lab4_hlg5155.Rmd Follow Section 2.2.2 to modify your YAML code Please make sure that your lab script has a floating table of contents (section 2.2.2, adding the toc_float: TRUE part) Or..OPTIONAL: If you want to explore some new formats, you can try one of the markdown templates stored in R.There are instructions on how to load them on the website here: https://github.com/juba/rmdformats/blob/master/README.md). Again, make sure to save your file as GEOG364_Lab5_PSU.ID.Rmd. Please also add in a floating table of contents to your YAML code. 6.1.3 Style guide A large component of the marks for your labs scripts focuses them being easily readable and easy to follow. Now you have had experience with R, here are some of the things we expect to get full marks: You include a floating table of contents, title and author in the YAML Code You include a “level 1” heading in your text for every lab challenge e.g. # Lab challenge 1 It’s easy to see where your answers are - so you have full sentences, not just one word answers. You have spell-checked your work! The spell check button is between the save button and the knit/preview button. You include blank lines before and after each code chunk, or new paragraph, or bullet point set or heading (put many blank lines in markdown files and R can automatically format them correctly). Any written answers are thoughtful and well considered. As you write your lab, regularly click the Preview or Knit Button and make sure everything looks correct and properly formatted. IF THERE ARE PROBLEMS, TALK TO AN INSTRUCTOR. 6.1.4 Download and run packages Follow the instructions in Section 3.2.2. to download and install the following packages tmap dismo Now add a new code chunk in your script. Inside add the following code and run it. library(tmap) # Used for plotting library(sp) # Used for the spsample function library(maptools) # Used for conversion from SPDF to ppp library(raster) # Used to clip out thiessen polygons library(gstat) # Use gstat&#39;s idw routine library(dismo) # Method for thiessen polygons library(knitr) # Used for the insert image function This needs to be at the top of your script because the library commands need to be run every time you open R. Now click the Preview or Knit Button and make sure everything looks correct. Don’t continue until you can make and view your html or nb.html file. If it doesn’t work, ask for help before moving on 6.2 Challenge 1: Explore the data (remember to create a new heading in your text for each challenge) 6.2.1 Download &amp; load the data Click on this link to download the data from the internet. Save it in your lab 5 folder. https://raw.githubusercontent.com/hgreatrex/GEOG364_Labs/master/Lab_5/Lab5California.rdata Now in a new code chunk, open the data in R using the load command e.g. load(\"Lab5California.rdata\") This command will load two California data-sets into R: California - The boundaries of the State of California Precip.table - The average rainfall recorded at weather stations in California during each month of the year. Each row represents one weather-station/gauge. For example, row 1 is the climatology for a weather station situated at Lat:36.47 and Long:-116.87. Columns The Long/Lat columns represent the longitude and latitude of each gauge The Alt column is the altitude/elevation in m. The Jan-Dec columns are the monthly average rainfall in mm and the Annual column is the total rainfall over the year in mm. 6.2.2 Initial analysis In a new code chunk, look at the summary of the Precip.table data (see previous labs for ideas on how to do this). Below your code chunk, in a full sentence, write the value of the maximum rainfall that fell in April across California, including units! We can also look at some summary plots of the data: In a new code chunk apply the hist command to create a histogram of January rainfall (e.g. apply hist to the Jan column of Precip.table). If you can’t remember how to select a column, see Lab 2, section 2.3.3. Copy the code below into R to make a series of boxplots. boxplot(Precip.table[,4:15], pch=4,cex=.6, cex.axis=.8,las=2, tcl=-.25) Underneath the code, explain: What pch, cex and las are doing. Typing ?par into the console might help What the grey box represents for each month (This might help: https://www.khanacademy.org/math/cc-sixth-grade-math/cc-6th-data-statistics/cc-6th-box-whisker-plots/v/reading-box-and-whisker-plots) Note, for your show me something new, you could consider making a fancy version of these boxplots, with many ideas here: https://www.r-graph-gallery.com/boxplot.html 6.2.3 Convert to spatial data Remember to add a heading.. this is your last reminder! You now need to convert your table of data to a spatial “sp” format. (If you wish to learn more about sp vs sf there is a data camp course) Copy the code chunk below into your code. Now replace the X-COORD and Y-COORD with the names of the columns that contain the x and y data in Precip.table. This is case sensitive and should be contained in quotes. #Convert to sp Precip.spdf &lt;- Precip.table coordinates(Precip.spdf) &lt;- c(&quot;X-COORD&quot;,&quot;Y-COORD&quot;) Check it works by plotting the data e.g. Figure 6.1: What your plot should look like If your plot does not look like this, DO NOT CONTINUE UNTIL IT IS FIXED. 6.2.4 Add a projection Now check the projection of both Precip.spdf and the California boundary data. (see Lab 2, section 2.5.1 if you have forgotten). You should see that Precip.spdf currently doesn’t have a map projection. As the x and y columns are longitude and latitude data, use the proj4string command to assign the projection of Precip.spdf to be “+proj=longlat +datum=WGS84 +no_defs +ellps=GRS80”. LOOK AT LAB 2 (section 2.5.5) FOR AN EXAMPLE OF HOW TO DO THIS For both the California boundary data and the precip data, we now need to set the projection to be something where distance is a conserved property. You could choose the UTM projection over California, but let’s see a new one: the Albers Equal-Area Conic projection (https://en.wikipedia.org/wiki/Albers_projection). The code for this is “+proj=aea +lat_1=34 +lat_2=40.5 +lat_0=0 +lon_0=-120 +x_0=0 +y_0=-4000000 +datum=NAD83 +units=km +ellps=GRS80” In a new code chunk, use the spTransform command twice to change the projection of each of Precip.spdf and the California data to the Albers projection. Again, Lab 2 section 2.5.2 might be useful here. If you get warnings like: “CRS object has comment, which is lost in output” or “NULL source CRS comment, falling back to PROJ string”, or anything else that specifically doesn’t say error,ignore them - they’re a consequence of the recent R upgrade. Now if you now run the plot command and also add California, they should plot on top of each other. DON’T CONTINUE UNTIL YOU CAN REPRODUCE THIS. Figure 6.2: What your plot should look like 6.3 Challenge 2: Exploratory analysis 6.3.1 Make plots in tmap In this section, you will learn a new package called tmap to make some more interesting plots. There is a general tutorial here: https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html I have provided the code for our example. Copy this into your script (you may have to re-write the \"s). Use the tutorial to help you write better comments to explain what is happening # WRITE COMMENT HERE. WHAT HAPPENS IF THIS IS SET TO &quot;plot&quot;? current.mode &lt;- tmap_mode(&quot;view&quot;) # Choose the name of the column you want to plot myColumn = &quot;Aug&quot; # WRITE COMMENT HERE. EXPLAIN WHAT THE CODE DOING tm_basemap(&quot;Stamen.Terrain&quot;) + tm_shape(Precip.spdf) + tm_dots(col=myColumn, palette = &quot;RdBu&quot;, title=paste(&quot;Total rain (mm):&quot;,myColumn), size=0.1,style=&quot;cont&quot;) # WRITE A COMMENT HERE TO EXPLAIN WHAT THE PASTE COMMAND IS DOING # This is based on leaflet, so you can change the interactive basemap with any from here: # http://leaflet-extras.github.io/leaflet-providers/preview/index.html # You can also run this command in the console to see all the colour palettes available to you: # tmaptools::palette_explorer() 6.3.2 Exploratory analysis Edit the code chunk above and run to look at the data for each month. You can just adjust the chunk above - you do not need to copy/paste or create new code chunks. From this exploratory analysis, briefly describe California’s rainfall patterns. For example, how does rainfall vary spatially in different months? What link is there to elevation? What factors you think might be causing the differences in rainfall amount across the State? Useful resource: you can also look in more detail at the climate for specific places here. https://weatherspark.com/ 6.4 Challenge 3: Thiessen polygons by hand Now we will move onto interpolation. At the moment we know that rainfall is field data - it has a value at every location. But we only have the values at our weather stations. It would be good to interpolate to new areas to create a map. The first technique we will use is Thiessen, or proximity polygons. These are also called Voronoi diagrams. In your text, briefly describe what Thiessen polygons are. Lectures and these resources might help: http://www.gitta.info/Accessibilit/en/html/UncProxAnaly_learningObject4.html On the separate sheet provided in the lab, calculate and draw the Thiessen polygons for the four points. Take a photo/scan of the sheet and load it into your lab as a screenshot. There are two ways to do this: Following these instructions if you prefer a vector approach: http://www.hydrology.bee.cornell.edu/BEE3710Handouts/ThiessenPoly.pdf Or a second approach is the raster version: for each grid-cell, work out which point it is closest to and build up polygons that way. We are grading you here on whether you explain what Theissen polygons are and if you attempt and upload your screenshot. Drawing these can be surprisingly hard, so we are not grading you on getting the polygons correct/perfect. “Cheating” is also encouraged! By this I mean: if you if you can work out how to reproduce these points in R, then you can automatically calculate the Thiessen polygons and see what the answer is.. 6.5 Challenge 4: Thiessen polygons in R The command to calculate Thiessen polygons in R comes from the dismo package and is called the voronoi function. It assumes that the data is a series of points in sp format. Apply the voronoi command to Precip.spdf and save it as a variable called Precip.Thiessen. If you need a hand, look at the helpfile for the voronoi function (and scroll to the end for an example) or see here https://rspatial.org/raster/rosu/Chapter11.html Copy/add the code below to your script. This plots the output of your voronoi analysis for individual columns of your data. AKA you can change “Annual” to be any column named in your Precip.Table. (try names(Precip.Table) in the console if you have forgotten them) spplot(Precip.Thiessen,&#39;Annual&#39;,col.regions=rev(get_col_regions())) #The spplot command is yet another way of plotting spatial data in R (there is a tutorial here: #http://www.nickeubank.com/wp-content/uploads/2015/10/RGIS3_MakingMaps_part1_mappingVectorData.html#spplot) Below the plot, explain what it is showing you. What do the colours represent and why are the polygons drawn that way. Why do the polygons look so strange towards the edge of the plot? It would be good to crop out California. We can use the raster package and the following code to do this. Run this code in your script. # Add the state boundaries CaliforniaNew &lt;- aggregate(California) Precip.Thiessen.clipped &lt;- intersect(Precip.Thiessen, CaliforniaNew) spplot(Precip.Thiessen.clipped, &#39;Annual&#39;, col.regions=rev(get_col_regions())) Below the code chunk, explain what are the limitations of Thiessen/proximity polygons, specifically for field data such as rainfall or elevation. What are the advantages and disadvantages of the final map? 6.6 Challenge 5: Inverse Distance Weighting info These resources should help you answer the questions below if needed The latest lecture notes O’Sullivan/Unwin textbook page 257-261 (the pdf is on the main-course Canvas under files/books) https://pro.arcgis.com/en/pro-app/help/analysis/geostatistical-analyst/how-inverse-distance-weighted-interpolation-works.htm https://mgimond.github.io/Spatial/spatial-interpolation.html In this section, describe the process of inverse distance weighting (IDW). You do not need to use equations. “IDW is an exact interpolator” Explain what this statement means. (O’Sullivan/Unwin might be helpful here) 6.7 Challenge 6: Inverse Distance Weighting code This code will run the inverse distance weighting code for the Annual rainfall California data, with a power of two. E.g. the weighting is proportional to \\[\\frac{1}{distance^{2}}\\] Copy the code below into R and make sure you can run it. It should give a plot of California. If there are errors about CRS, ignore them. # Create an empty grid over California grid &lt;- as.data.frame(spsample(Precip.spdf, &quot;regular&quot;, n=50000)) names(grid) &lt;- c(&quot;X&quot;, &quot;Y&quot;) coordinates(grid)&lt;- c(&quot;X&quot;, &quot;Y&quot;) gridded(grid) &lt;- TRUE # Create SpatialPixel object fullgrid(grid) &lt;- TRUE # Create SpatialGrid object # Add the California projection information to the empty grid proj4string(grid) &lt;- proj4string(California) # What inverse distance weighting power would you like? Power &lt;- 2 # The IDW command calculates the inverse distance weighting Precip.idw &lt;- gstat::idw(Annual ~ 1, # Choose the annual column, no trend Precip.spdf, # For the precip sp data newdata=grid,# interpolate onto the new grid idp=Power) # Set the power to be the number stored in the Power variable # Now convert to a raster object &amp; clip to California Precip.idw.raster &lt;- raster(Precip.idw) Precip.idw.mask &lt;- mask(Precip.idw.raster, California) # And plot to a static plot current.mode &lt;- tmap_mode(&quot;plot&quot;) # A more complex tmap tm_shape(Precip.idw.mask) + tm_raster(n=10,palette = &quot;RdBu&quot;, style=&quot;cont&quot;, breaks=seq(from=0,to=2400,length.out=20), title=paste(&quot;Rain, Power:&quot;,Power)) + tm_shape(Precip.spdf) + tm_dots(size=0.02) + tm_legend(legend.outside=TRUE) Below the code chunk, answer the following questions. Please include the questions in your text to make it easy for us to grade your answers. In the output above, for a power of 2, there appear to be “circles” or “bulls-eyes” around each many of the rain-gauges. Why is this? What happens when you make the power a high number, say 12, and rerun the code chunk? Why does the map look the way it does? What happens when you make the power to a low number below 1, say 0.5, and rerun the code chunk? Why does the map look the way it does? 6.8 Challenge 7: Checking accuracy As described in https://mgimond.github.io/Spatial/spatial-interpolation.html Finding the “best” power can be subjective. . One option is to split the points into two sets: we do the interpolation using half the points, then check to see how good it was using the other half of the data. The downside of this is that we are only using half the data, so we might leave out important information. This is a loss of statistical power. A better approach to fine-tune the choice of the power parameter, we can perform a leave-one-out validation routine to measure the error in the interpolated values. Here, we remove one data point from the dataset and estimate its value using all other points and the IDW tecnique. Then we repeat this process for every point. The interpolated values are then compared with the actual values from the omitted point. This method is sometimes referred to as jackknifing or cross-validation. We can then create a scatterplot of the predicted vs. expected precipitation values from our dataset. The solid diagonal line represents the one-to-one slope (i.e. if the predicted values matched the true values exactly, then the points would fall on this line). The red dashed line is a linear fit to the points which is here to help guide our eyes along the pattern generated by these points. We can then calculate the RMSE, the Root Mean Squared Error, to see how far the residuals are from the line. The smaller the RMSE, the better the fit. The code below shows the plot and RMSE for the California data and a power of 2. # Change the power here and don&#39;t touch the rest power=2 # Leave-one-out validation routine IDW.out &lt;- vector(length = length(Precip.spdf)) for (i in 1:length(Precip.spdf)) { IDW.out[i] &lt;- suppressWarnings( gstat::idw(Annual ~ 1, Precip.spdf[-i,], Precip.spdf[i,], idp=power,debug.level=0)$var1.pred) } # Compute RMSE RMSE &lt;- sqrt( sum((IDW.out - Precip.spdf$Annual)^2) / length(Precip.spdf)) # Plot the differences plot(IDW.out ~ Precip.spdf$Annual, asp=1, xlab=&quot;Observed&quot;, ylab=&quot;Predicted&quot;, pch=16, col=rgb(0,0,0,0.5),main=paste(&quot;RMSE:&quot;,round(RMSE))) abline(lm(IDW.out ~ Precip.spdf$Annual), col=&quot;red&quot;, lw=2,lty=2) abline(0,1) Edit the power and keep reruning the code chunk above until you think you have found the lowest RMSE and the “best” fit. Hint - the power does not need to be an integer. It could be 2.4. Below the code chunk confirm the final power that you chose. For your show me something new, you are welcome to create a for loop that does this automatically). Have a look at the IDW map with your final power (by rerunning the earlier code chunk that made the map). Do you agree that this was the best fit? What are the strengths of this interpolation technique and what are the weaknesses? 6.9 Challenge 8: Your own IDW Finally, copy the code in lab challenge 6 into a new code chunk.Now modify it to show create an IDW map with a power of 2 for the elevation data (your Alt column in Precip.spdf) Your final plot should be properly captioned (e.g. the legend should be talking about elevation in m) and you will have to change the breaks on the color scale to be something that fits the data. If you don’t get this working, don’t panic, simply comment out the code chunk by putting a # in front of each line - then your file will still knit and we can see how far you got. 6.10 Challenge 9 Show me something new Please describe what you are trying to achieve. For 2.5/5 marks 3 NEW chapters of a data camp course and include completion screen-shots in your lab script You can make some minor changes, for example, you could use this equation editor - https://www.codecogs.com/latex/eqneditor.php - and this tutorial - https://www.math.mcgill.ca/yyang/regression/RMarkdown/example.html, you could include a mathematical equation of your choice in your markdown text. You could make some minor changes to some of the code, for example making tmap plots with a new color palette. OR for 5/5 you can continue to do the classic “something new”, where you need to demonstrates the use of a function or package that was not specifically covered in the handout, lecture, or lab. Remember you actually have to do something new, not repeat what you did in previous weeks Some ideas were included throughout the lab, for example - You could use ggplot2 to do the Thessian polygons instead: https://cran.r-project.org/web/packages/ggvoronoi/vignettes/ggvoronoi.html - You could do a linear regression between annual rainfall and elevation - You could explore the tmap and spplot tutorials and see other things you could do. - More spatial ideas which might work on our data here: https://rspatial.org/raster/rosu/Chapter11.html https://tlorusso.github.io/geodata_workshop/tmap_package. For these, you need to make a meaningful change and add a new feature to the map to get 5/5. - You could convert the data to ppp and do one aspect of point pattern analysis on the dataset - You successfully managed to write R code to create Thiessen polygons on the small dataset in challenge 3.1. If you want a larger challenge (also for 5/5 but building your skills) - You could look at doing a first order polynomial fit: https://mgimond.github.io/Spatial/interpolation-in-r.html#st-order-polynomial-fit - You could explore the akima package for interpolation - Applying a for loop to the IDW data (the intermediate R datacamp course helps) - There are lots of ideas in the previous lab suggestions. 6.11 Lab-5 submission check This lab is designed to be relatively short, so that you can start thinking about your final projects and your own spatial data. I will discuss this in future lectures. For this lab, here is the mark breakdown: HTML FILE SUBMISSION - 10 marks RMD CODE SUBMISSION - 10 marks WORKING CODE - 10 marks: Your code works and the output of each code chunk is included in the html file output (e.g. you pressed run-all before you finished) EASY TO READ LAB SCRIPT - 10 marks: You have followed the style guide in section 6.1. CHALLENGE 1 - 10 marks: You answered all the questions, made your histogram/boxplots &amp; converted your data correctly to spatial data CHALLENGE 2 - 5 marks: You thoughtfully used the tmap plots to explore the spatial structure of rainfall over California in different months CHALLENGE 3 &amp; 4 - 15 marks: You uploaded your “by hand” attempt at the polygons and described what they are. You then calculated them in R and described their limitations. CHALLENGE 5 - 5 marks: You explained the process of inverse distance weighting and what was meant by an exact interpolator. CHALLENGE 6 - 10 marks: You answered the questions to explain the spatial features in the data. To get full marks you need to refer back to what IDW is actually doing.. CHALLENGE 7 &amp; 8 - 10 marks: You thoughtfully commented on the final power chosen for IDW and the strengths and weaknesses of this technique. You then managed to create an IDW analysis for the elevation column. SOMETHING NEW - 5 marks You made a minor change to an existing piece of code (2.5/5) or did 3 data camp chapters. OR You demonstrated the use of a function or concept that was not specifically covered in the handout, lecture, or lab [100 marks total] "],
["lab-6.html", "Chapter 7 Lab 6 7.1 Lab 6 Set-Up 7.2 Tutorial 1: Spatial weights and Join Counts 7.3 Challenge 1: Join Counts 7.4 Tutorial 2: Social vulnerability and Moran’s I 7.5 Challenge 2: Your own spatial analysis 7.6 Challenge 3 Show me something new 7.7 Lab-5 submission check", " Chapter 7 Lab 6 Objective: Welcome to Lab 6, where we are going to focus on spatial autocorrelaton, join counts and Moran’s I. Specifically you will: Read in shapefiles Have the opportunity to make spatial weights matrices Explore the tmap package Look at join counts data Create Moran’s I plots and interpret them Data-sets: You will be conducting your analysis on some toy data, then examining your own US based county vulnerability data from the CDC. Lab structure: Now you are getting more experienced in R, I will provide a worked example then get you to do something similar on your own data. Although you are of course welcome to copy/paste/run the worked example, YOU SHOULD NOT INCLUDE THE WORKED EXAMPLE IN YOUR FINAL LAB REPORT. 7.1 Lab 6 Set-Up 7.1.1 Create your Lab 6 project file Open a new version of R-studio. Click the file menu, then new project in a new directory. Select your 364 directory, then name the project Lab 6. If you are stuck on this process, see the start of previous labs. You can check this has worked by looking on the file explorer on your computer, then selecting your GEOG364 folder and checking that a Lab 6 folder has appeared with your Lab4.Proj file inside it. 7.1.2 Create your NoteBook file Here you have a choice: Either.. you can create a standard lab script as before: Go to File/New File and create a new R-NOTEBOOK file. Delete the friendly text (everything from line 6 onward) Save your file as GEOG364_Lab6_PSU.ID.Rmd e.g. GEOG364_Lab6_hlg5155.Rmd Follow Section 2.2.2 to modify your YAML code Please make sure that your lab script has a floating table of contents (section 2.2.2, adding the toc_float: TRUE part) Or..OPTIONAL: If you want to explore some new formats, you can try one of the markdown templates stored in R.There are instructions on how to load them on the website here: https://github.com/juba/rmdformats/blob/master/README.md). Again, make sure to save your file as GEOG364_Lab6_PSU.ID.Rmd. Please also add in a floating table of contents to your YAML code. 7.1.3 Style guide A large component of the marks for your labs scripts focuses them being easily readable and easy to follow. Now you have had experience with R, here are some of the things we expect to get full marks: You include a floating table of contents, title and author in the YAML Code You include a “level 1” heading in your text for every lab challenge e.g. # Lab challenge 1 It’s easy to see where your answers are - so you have full sentences, not just one word answers. You have spell-checked your work! The spell check button is between the save button and the knit/preview button. You include blank lines before and after each code chunk, or new paragraph, or bullet point set or heading (put many blank lines in markdown files and R can automatically format them correctly). Any written answers are thoughtful and well considered. As you write your lab, regularly click the Preview or Knit Button and make sure everything looks correct and properly formatted. IF THERE ARE PROBLEMS, TALK TO AN INSTRUCTOR. 7.1.4 Download and run packages Follow the instructions in Section 3.2.2. to download and install the following packages spdep USAboundaries plus any others below that you are missing. Now add a new code chunk in your script. Inside add the following code and run it. library(rgdal) library(sp) library(sf) library(spdep) ## Loading required package: spData ## To access larger datasets in this package, install the spDataLarge ## package with: `install.packages(&#39;spDataLarge&#39;, ## repos=&#39;https://nowosad.github.io/drat/&#39;, type=&#39;source&#39;)` library(tmap) library(raster) library(USAboundaries) This needs to be at the top of your script because the library commands need to be run every time you open R. Now click the Preview or Knit Button and make sure everything looks correct. Don’t continue until you can make and view your html or nb.html file. If it doesn’t work, ask for help before moving on 7.2 Tutorial 1: Spatial weights and Join Counts First, read the pdf on join counts from canvas - it’s only 3 pages but provides crucial background 7.2.1 Making the grid Join count and autocorrelation statistics are valuable in understanding spatial dependencies among sample units. This first tutorial focuses on using R to calculate join count statistics using a toy dataset. The code below creates a grid, or matrix of data that we can use to test Joins Counts. Here’s a picture of the grid we are going to make. In this case, each polygon is a simple grid cell. Now, I will make the data in R using a new type of data called a matrix (see data camp intro R for more details) row.1 &lt;- rep(1,6) row.2 &lt;- c(0,1,1,1,1,1) row.3 &lt;- c(rep(0,5),1) row.4 &lt;- rep(0,6) row.5 &lt;- c(0,0,0,1,0,0) row.6 &lt;- rep(0,6) matrixA &lt;- matrix(c(row.1, row.2, row.3, row.4, row.5, row.6), nrow=6,ncol=6, byrow=TRUE) matrixA ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 1 1 1 1 1 ## [2,] 0 1 1 1 1 1 ## [3,] 0 0 0 0 0 1 ## [4,] 0 0 0 0 0 0 ## [5,] 0 0 0 1 0 0 ## [6,] 0 0 0 0 0 0 and then we then convert it to spatial data using the raster command. rasterA &lt;- raster(matrixA) plot(rasterA) text(coordinates(rasterA), labels=rasterA[], cex=1.5) 7.2.2 Creating the spatial weights Our raster data will now easily convert to vector data so that we can determine which grid-cells are “nearby”. To find adjacent polygons, we can use package ‘spdep’. First, we covert our raster data to a spatial polygon and it’s coordinates Then we use the poly2nb command to work out the rook’s adjacency Then we plot the data Finally, we calculate the weights matrix. In this case we use a binary (B) criteria, i.e. there is adjacency (1) or there is no adjacency (0). polyA &lt;- rasterToPolygons(rasterA, dissolve=FALSE) spA &lt;- SpatialPolygons(polyA@polygons) nbA.rook &lt;- poly2nb(spA, queen = FALSE) plot(spA, border=&#39;blue&#39;) plot(nbA.rook, coordinates(spA), col=&#39;red&#39;, lwd=2, add=TRUE) weightsA.rook &lt;- nb2listw(nbA.rook, style=&#39;B&#39;) 7.2.3 Conducting a join-counts analysis Now, everything is set for the analyses: We have our raster, which shows the presence/absence of green grid cells, plus the weights information (as object ‘weightsA.rook’). Finally, we apply the Join Count test to evaluate presence or absence or spatial autocorrelation. This is done using a z-test. Slightly differently to the textbook and the lecture notes (which looked for green/white), the R version of this test looks for how likely it is that there are white-to-white joins compared to random chance, but the principle is the same. So we have: H0: The pattern exhibits complete spatial randomness or is dispersed. The number of Observed “same color” joins (OWhite-White) is no higher than what you would expect from an Independent Random Process: OWW = EWW H1: The pattern is not random. The number of “same color” joins is unusually high (AKA the pattern is clustered): OWW &gt; EWW Here is the code for the Z-test jc_testA &lt;- joincount.test(fx= as.factor(polyA$layer), #data listw= weightsA.rook, #weights alternative=&quot;greater&quot;) Another thing to note is that you can select which different tailed alternative hypothesis: (default) ‘greater’: the alternative hypothesis that the number of like joins is more than expected by random chance. (THIS IS ONLY TRUE IF THE THING YOU ARE TESTING IS WW or GG joins. The opposite would be the case if you tested GW joins, think through why this is the case) ‘less’: the alternative hypothesis that the number of like joins is fewer than expected by random chance (indicating higher levels of dispersion). (IF THE THING YOU ARE TESTING IS WW or GG joins. The opposite would be the case if you tested GW joins.) ‘two.sided’: the alternative hypothesis that the number of like joins is simply different to the number you would expect from random chance. This is unusual to use as we are normally looking for either clustering or dispersion. joincount.test actually does two tests, so we use double square brackets [[ ]] to show the first and second test separately: # White-to-white Join counts jc_testA[[1]] ## ## Join count test under nonfree sampling ## ## data: as.factor(polyA$layer) ## weights: weightsA.rook ## ## Std. deviate for 0 = 4.1544, p-value = 1.631e-05 ## alternative hypothesis: greater ## sample estimates: ## Same colour statistic Expectation Variance ## 33.000000 24.095238 4.594478 # Green-to-green Join counts jc_testA[[2]] ## ## Join count test under nonfree sampling ## ## data: as.factor(polyA$layer) ## weights: weightsA.rook ## ## Std. deviate for 1 = 4.06, p-value = 2.453e-05 ## alternative hypothesis: greater ## sample estimates: ## Same colour statistic Expectation Variance ## 15.000000 7.428571 3.477764 We can see that there are many more white-white joins / green-green joins than you might expect. In fact, looking at the low p-value, we can see that it is very unusual to see so many same-color joins, so in this case, most people can can safely reject the null hypothesis. 7.3 Challenge 1: Join Counts 7.3.1 Challenge 1a Have you read the pdf on join counts from canvas? It’s only 3 pages but provides crucial background Start a new section called Challenge 1, and a new level 2 sub heading called Challenge 1a. Underneath, summarise what spatial autocorrelation is, referencing: Tobler’s law, What is a global autocorrelation measure What is a local autocorrelation measure 7.3.2 Challenge 1b Make a new sub-heading called challenge 1b. Copy/use the code in the tutorial above to conduct your own analysis on your own new matrix that you create called matrixB. Try to make a grid that has is likely to have negative spatial autocorrelation, but still has a little bit of randomness. Use a queen’s weights matrix in your example You should conduct a hypothesis test at a significance level of p-value:5% to see if the data is unusually clustered and interpret what this means. As part of your answer, you should also explain what the rep function which is included in the tutorial. To get full marks you need neat code, comments and good variable names (we’re doing grid B not grid A!). 7.4 Tutorial 2: Social vulnerability and Moran’s I This next tutorial applies what we have learned to a real life scenario, that of vulnerability modelling in the USA. The Centers for Disease Control and Prevention Social Vulnerability Index (SVI) was created to help public health officials and emergency response planners identify and map the communities that will most likely need support before, during, and after a hazardous event. SVI indicates the relative vulnerability of every U.S. Census tract. Census tracts are subdivisions of counties for which the Census collects statistical data. SVI ranks the tracts on 15 social factors, including unemployment, minority status, and disability, and further groups them into four related themes. Thus, each tract receives a ranking for each Census variable and for each of the four themes, as well as an overall ranking. I am going to use this data for a specific state to look at the spatial distribution of some of these variables. 7.4.1 Download the data First I will find and download the data. I found the SVI data at https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html The documentation to understand what the column headings of the data can be found at: https://www.atsdr.cdc.gov/placeandhealth/svi/documentation/pdf/SVI2018Documentation-H.pdf I then downloaded county level data for Florida for 2018 as an ESRI shapefile. I saved this to my lab 6 folder, then unzipped it. 7.4.2 Read the data into R The readOGR command will read a shapefile into R. You should split your command into the “dsn”, the folder containing the data and the “layer”, the name of the file with NO extension In our case, we are using the Lab 6 r-project to set our folder (the “.” below), then we just enter the name of the sub-folder and the file. If you can’t make it work, then running file.choose() in the console is a good way to get the correct wording. NOTE THERE IS NO FORWARD SLASH / DOUBLE BACKSLASH AT THE END OF THE DSN BIT # This reads in the shapefile containing the SVI Data and saves it to an sp variable called SVI SVI &lt;- readOGR(dsn=&quot;./Florida_COUNTY&quot;, layer=&quot;SVI2018_FLORIDA_county&quot;) For a windows machine, this might look like # the dot means start in the current folder (lab 6), then move into the Florida county sub-folder SVI &lt;- readOGR(dsn=&quot;.\\\\Florida_COUNTY&quot;, layer=&quot;SVI2018_FLORIDA_county&quot;) # or if you use file.choose() for the full path - this is the one on Saumya&#39;s computer - YOURS WILL LOOK DIFFERENT AND NOT BE ON THE E DRIVE SVI &lt;- readOGR(dsn=&quot;E:\\\\GEOG364\\\\Lab6\\\\Florida_COUNTY&quot;, layer=&quot;SVI2018_FLORIDA_county&quot;) # or for Mac OR PC, if the files are in your lab 6 folder and you are running R from the project file, you don&#39;t need the dsn at all as R is already looking in the right place SVI &lt;- readOGR(&quot;SVI2018_FLORIDA_county&quot;) I will also use the us_states command from the USAboundaries package to download the border of Florida. # This downloads the US state border data for my state, it&#39;s from the USAboundaries package State.border &lt;- us_states(states = &quot;Florida&quot;) # convert to sp State.border &lt;- as(State.border,Class=&quot;Spatial&quot;) 7.4.3 Change the projection Now let’s check the projection of both and make sure they are the same. projection(SVI) ## [1] &quot;+proj=longlat +datum=NAD83 +no_defs&quot; projection(State.border) ## [1] &quot;+proj=longlat +datum=WGS84 +no_defs&quot; Although they are both equal, I’m now going to change them both to a UTM projection as it’s more robust. I found the code by selecting Florida UTM here: https://mangomap.com/robertyoung/maps/69585/what-utm-zone-am-i-in-# Then checking the epsg code here: http://epsg.io/ # I&#39;m using the suppressWarnigngs command to get rid of those CRS messages/warnings SVI &lt;- suppressWarnings(spTransform(SVI,CRS(&quot;+init=epsg:3747&quot;))) State.border &lt;- suppressWarnings(spTransform(State.border,CRS(&quot;+init=epsg:3747&quot;))) and check it worked: projection(SVI) ## [1] &quot;+proj=utm +zone=17 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; projection(State.border) ## [1] &quot;+proj=utm +zone=17 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; 7.4.4 Explore the data Now I can explore the dataset. I have already looked at the documentation and seen that there are many columns I can look at. I can see these by showing the column names here: names(SVI) ## [1] &quot;ST&quot; &quot;STATE&quot; &quot;ST_ABBR&quot; &quot;COUNTY&quot; &quot;FIPS&quot; ## [6] &quot;LOCATION&quot; &quot;AREA_SQMI&quot; &quot;E_TOTPOP&quot; &quot;M_TOTPOP&quot; &quot;E_HU&quot; ## [11] &quot;M_HU&quot; &quot;E_HH&quot; &quot;M_HH&quot; &quot;E_POV&quot; &quot;M_POV&quot; ## [16] &quot;E_UNEMP&quot; &quot;M_UNEMP&quot; &quot;E_PCI&quot; &quot;M_PCI&quot; &quot;E_NOHSDP&quot; ## [21] &quot;M_NOHSDP&quot; &quot;E_AGE65&quot; &quot;M_AGE65&quot; &quot;E_AGE17&quot; &quot;M_AGE17&quot; ## [26] &quot;E_DISABL&quot; &quot;M_DISABL&quot; &quot;E_SNGPNT&quot; &quot;M_SNGPNT&quot; &quot;E_MINRTY&quot; ## [31] &quot;M_MINRTY&quot; &quot;E_LIMENG&quot; &quot;M_LIMENG&quot; &quot;E_MUNIT&quot; &quot;M_MUNIT&quot; ## [36] &quot;E_MOBILE&quot; &quot;M_MOBILE&quot; &quot;E_CROWD&quot; &quot;M_CROWD&quot; &quot;E_NOVEH&quot; ## [41] &quot;M_NOVEH&quot; &quot;E_GROUPQ&quot; &quot;M_GROUPQ&quot; &quot;EP_POV&quot; &quot;MP_POV&quot; ## [46] &quot;EP_UNEMP&quot; &quot;MP_UNEMP&quot; &quot;EP_PCI&quot; &quot;MP_PCI&quot; &quot;EP_NOHSDP&quot; ## [51] &quot;MP_NOHSDP&quot; &quot;EP_AGE65&quot; &quot;MP_AGE65&quot; &quot;EP_AGE17&quot; &quot;MP_AGE17&quot; ## [56] &quot;EP_DISABL&quot; &quot;MP_DISABL&quot; &quot;EP_SNGPNT&quot; &quot;MP_SNGPNT&quot; &quot;EP_MINRTY&quot; ## [61] &quot;MP_MINRTY&quot; &quot;EP_LIMENG&quot; &quot;MP_LIMENG&quot; &quot;EP_MUNIT&quot; &quot;MP_MUNIT&quot; ## [66] &quot;EP_MOBILE&quot; &quot;MP_MOBILE&quot; &quot;EP_CROWD&quot; &quot;MP_CROWD&quot; &quot;EP_NOVEH&quot; ## [71] &quot;MP_NOVEH&quot; &quot;EP_GROUPQ&quot; &quot;MP_GROUPQ&quot; &quot;EPL_POV&quot; &quot;EPL_UNEMP&quot; ## [76] &quot;EPL_PCI&quot; &quot;EPL_NOHSDP&quot; &quot;SPL_THEME1&quot; &quot;RPL_THEME1&quot; &quot;EPL_AGE65&quot; ## [81] &quot;EPL_AGE17&quot; &quot;EPL_DISABL&quot; &quot;EPL_SNGPNT&quot; &quot;SPL_THEME2&quot; &quot;RPL_THEME2&quot; ## [86] &quot;EPL_MINRTY&quot; &quot;EPL_LIMENG&quot; &quot;SPL_THEME3&quot; &quot;RPL_THEME3&quot; &quot;EPL_MUNIT&quot; ## [91] &quot;EPL_MOBILE&quot; &quot;EPL_CROWD&quot; &quot;EPL_NOVEH&quot; &quot;EPL_GROUPQ&quot; &quot;SPL_THEME4&quot; ## [96] &quot;RPL_THEME4&quot; &quot;SPL_THEMES&quot; &quot;RPL_THEMES&quot; &quot;F_POV&quot; &quot;F_UNEMP&quot; ## [101] &quot;F_PCI&quot; &quot;F_NOHSDP&quot; &quot;F_THEME1&quot; &quot;F_AGE65&quot; &quot;F_AGE17&quot; ## [106] &quot;F_DISABL&quot; &quot;F_SNGPNT&quot; &quot;F_THEME2&quot; &quot;F_MINRTY&quot; &quot;F_LIMENG&quot; ## [111] &quot;F_THEME3&quot; &quot;F_MUNIT&quot; &quot;F_MOBILE&quot; &quot;F_CROWD&quot; &quot;F_NOVEH&quot; ## [116] &quot;F_GROUPQ&quot; &quot;F_THEME4&quot; &quot;F_TOTAL&quot; &quot;E_UNINSUR&quot; &quot;M_UNINSUR&quot; ## [121] &quot;EP_UNINSUR&quot; &quot;MP_UNINSUR&quot; &quot;E_DAYPOP&quot; &quot;Shape_STAr&quot; &quot;Shape_STLe&quot; There are lots more non-spatial plots I can make, but for now I will move onto maps. Today I am going to use tmap. As you can see, a plot is built up in layers. Each tm_shape line identifies some data you want to plot, then the indented lines afterwards plot it. I have chosen to plot one of the many variables that are available to me:E_TOTPOP. From the documentation I found this represents the population total in each county. tm_shape(SVI) + tm_polygons(col=&quot;E_TOTPOP&quot;, style=&quot;quantile&quot;, border.col = &quot;black&quot;, palette=&quot;Spectral&quot;) + tm_legend(outside = TRUE) + tm_shape(State.border) + tm_borders() + tm_layout(main.title = &quot;Population&quot;, main.title.size = 0.95) ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output ## Warning: Number of levels of the variable &quot;E_TOTPOP&quot; is 67, which is ## larger than max.categories (which is 30), so levels are combined. Set ## tmap_options(max.categories = 67) in the layer function to show all levels. Now we have a new problem (this is real life data!). I expected the data to be numeric - e.g. the average annual income in dollars, but R has not understood this. This is why the colorbar looks weird and why R is giving me the warning message about “too many breaks”, when I made the plot above. If I print SVI$E_TOTPOP onto the screen then I noticed that each one has quote marks around it, so R thinks its text. print(SVI$E_TOTPOP) ## [1] &quot;235503&quot; &quot;80578&quot; &quot;120999&quot; &quot;412144&quot; &quot;107139&quot; &quot;455086&quot; &quot;170442&quot; ## [8] &quot;157581&quot; &quot;576808&quot; &quot;200737&quot; &quot;31877&quot; &quot;150984&quot; &quot;176954&quot; &quot;207291&quot; ## [15] &quot;76325&quot; &quot;311522&quot; &quot;263148&quot; &quot;373853&quot; &quot;957875&quot; &quot;527634&quot; &quot;182696&quot; ## [22] &quot;14105&quot; &quot;363922&quot; &quot;16055&quot; &quot;65858&quot; &quot;718679&quot; &quot;288102&quot; &quot;143087&quot; ## [29] &quot;335362&quot; &quot;27785&quot; &quot;510593&quot; &quot;182482&quot; &quot;1446277&quot; &quot;17615&quot; &quot;348371&quot; ## [36] &quot;305591&quot; &quot;1909151&quot; &quot;39961&quot; &quot;11736&quot; &quot;1321194&quot; &quot;1378883&quot; &quot;924229&quot; ## [43] &quot;14444&quot; &quot;338619&quot; &quot;69105&quot; &quot;8365&quot; &quot;16437&quot; &quot;102101&quot; &quot;15239&quot; ## [50] &quot;24566&quot; &quot;26979&quot; &quot;22098&quot; &quot;668671&quot; &quot;13363&quot; &quot;19430&quot; &quot;2715516&quot; ## [57] &quot;43924&quot; &quot;40572&quot; &quot;18474&quot; &quot;72766&quot; &quot;48472&quot; &quot;36399&quot; &quot;27228&quot; ## [64] &quot;46017&quot; &quot;8744&quot; &quot;40127&quot; &quot;14269&quot; Let’s use the as.numeric command to turn that column back into numbers SVI$E_TOTPOP &lt;- as.numeric(SVI$E_TOTPOP) print(SVI$E_TOTPOP) ## [1] 235503 80578 120999 412144 107139 455086 170442 157581 576808 ## [10] 200737 31877 150984 176954 207291 76325 311522 263148 373853 ## [19] 957875 527634 182696 14105 363922 16055 65858 718679 288102 ## [28] 143087 335362 27785 510593 182482 1446277 17615 348371 305591 ## [37] 1909151 39961 11736 1321194 1378883 924229 14444 338619 69105 ## [46] 8365 16437 102101 15239 24566 26979 22098 668671 13363 ## [55] 19430 2715516 43924 40572 18474 72766 48472 36399 27228 ## [64] 46017 8744 40127 14269 and now when we plot.. tm_shape(SVI) + tm_polygons(col=&quot;E_TOTPOP&quot;, style=&quot;pretty&quot;, border.col = &quot;black&quot;, palette=&quot;Spectral&quot;) + tm_legend(outside = TRUE) + tm_shape(State.border) + tm_borders() + tm_layout(main.title = &quot;Total population&quot;, main.title.size = 0.95) ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output That looks better! So it seems that my population is clustered in the south and mid-way up state. This makes sense given that the blue area is the Miami-Dade district. To better understand my data, there are two other ways I might want to use tmap: Firstly, I might want to compare two maps at once. I can do this by creating two separate maps and instead of plotting them, simply saving them as Map1 and Map2, then plotting them together using the tmap_arrange command. I might also want to zoom in on these in the way we earlier made leaflet plot. In this case, we simply switch map view from “plot” to “view”. I also added a leaflet basemap (code from here: http://leaflet-extras.github.io/leaflet-providers/preview/) For other color palettes, see here: https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing # create map 1 Map1 &lt;- tm_basemap(&quot;OpenStreetMap.Mapnik&quot;) + # Set the watercolor basemap tm_shape(SVI) + # plot the SVI data tm_polygons(col=&quot;E_TOTPOP&quot;, # name of the column I am plotting style=&quot;pretty&quot;, # do i want color breaks, or continous color bar? border.col = NULL, # no outlines palette=&quot;Spectral&quot;, #color palette title=&quot;Population Density&quot;, #plot title alpha = .5) + #slightly transparent tm_shape(State.border) + # Plot the State.border data tm_borders() #plot it as borders/lines # remember to convert any new numeric variables into numbers SVI$E_PCI &lt;- as.numeric(SVI$E_PCI) # create map 2 Map2 &lt;- tm_basemap(&quot;Stamen.Watercolor&quot;) + tm_shape(SVI) + tm_polygons(col=&quot;E_PCI&quot;,style=&quot;pretty&quot;, border.col = NULL, palette=&quot;RdBu&quot;, title=&quot;Average income&quot;, alpha = .7) + tm_shape(State.border) + tm_borders() # plot them both tmap_arrange(Map1,Map2) ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output 7.4.5 Creating a spatial weights matrix For the rest of this lab I am going to focus on population density. First, I look to see if the population density data looks like it clusters or is dispersed. I can also look at other data to understand why this might be the case (e.g. is there some underlying reason why our data looks this way). From my perspective and the color scale, it does look like there is clustering of high population areas. As described in the first part of the lab, before we can formally model the spatial dependency shown in the above map, we must first cover how neighborhoods are spatially connected to one another. That is, what does “near” mean when we say “near things are more related than distant things”? For each census tract, we need to define What counts as its neighbor (connectivity) How much does each neighbor matter? (spatial weights) To do this we again calculate the spatial weights matrix: # calculate the spatial weights matrix spatial.matrix.rook &lt;-poly2nb(SVI, queen=F) plot(SVI, border=&#39;blue&#39;) plot(spatial.matrix.rook, coordinates(SVI), col=&#39;black&#39;, lwd=2, add=TRUE) 7.4.5.1 Moran’s I scatterplot We can then assign weights to each one based on whether it is a neighbour or not. Again, I am just using rook’s contingency, a binary classification of 1 if a county is a neighbour and 0 if not. # calculate the spatial weights weights.rook &lt;- nb2listw(spatial.matrix.rook, style=&#39;B&#39;) and now I will calculate the Moran’s scatterplot ## and calculate the moran&#39;s plot moran.plot(SVI$E_TOTPOP, weights.rook, xlab = &quot;Population&quot;, ylab = &quot;Neighbors Population&quot;, labels=SVI$COUNTY) Here we can see that high population counties tend to be clustered together, but that there is not a perfect relationship. Miami Dade in particular is skewing the relationship. The “Moran’s I” global statistic is essentially a summary of this plot. We look at the correlation coeffient between the two variables. Figure 7.1: plot from: https://mgimond.github.io/Spatial/spatial-autocorrelation.html The easiest way to see this in R is to use the Moran.test command, which also assesses the significance of your result: moran.test(SVI$E_TOTPOP, weights.rook) ## ## Moran I test under randomisation ## ## data: SVI$E_TOTPOP ## weights: weights.rook ## ## Moran I statistic standard deviate = 4.205, p-value = 1.305e-05 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.297170683 -0.015151515 0.005516543 We find that the Moran’s I is positive (0.30) and statistically significant (p-value &lt; 0.05). Remember from lecture that the Moran’s I is simply a correlation, and correlations go from -1 to 1. A 0.30 correlation is relatively high, indicating some clustering, but not a very strong signal. Moreover, we find that this correlation is statistically significant (p-value basically at 0). 7.5 Challenge 2: Your own spatial analysis Make a new heading challenge 2. In here, I would like you to present a spatial autocorrelation analysis of your own. This will be easier in week 2 after the lecture on Moran’s I. Your job is to repeat this tutorial in your own .rmd file but for a DIFFERENT STATE and for a DIFFERENT VARIABLE. (not Florida and not Population density) Your write up should include: A description of what the SVI data is and why it’s important (remember the documentation will help) Why you chose the state that you did. Is there something interesting there you want to look at? All the relevant code to make the analysis work for you. What map projection you chose and why. An exploratory analysis of the data and an explanation of why you chose your variable, or your initial thoughts on its spatial distribution (remember the documentation will help) A Moran’s analysis with the results explained clearly in the text. For full grades, the code will also show some originality e.g. making the plots feel like “yours” (choosing your own color palettes, map backgrounds etc), including markdown elements such as references or equations as relevant, including section sub-headings and thoughtful analysis. 7.6 Challenge 3 Show me something new Please describe what you are trying to achieve. For 2.5/5 marks 3 NEW chapters of a data camp course and include completion screen-shots in your lab script You can make some minor changes, for example, you could use this equation editor - https://www.codecogs.com/latex/eqneditor.php - and this tutorial - https://www.math.mcgill.ca/yyang/regression/RMarkdown/example.html, you could include a mathematical equation of your choice in your markdown text. You could make some minor changes to some of the code, for example adding some new functionality to tmap. OR for 5/5 you can continue to do the classic “something new”, where you need to demonstrates the use of a function or package that was not specifically covered in the handout, lecture, or lab. Remember you actually have to do something new, not repeat what you did in previous weeks My strong recommendation is to do something cool with the data you have. For example, you have loads of data you can make scatterplots with. The easiest way to do this is to make a new table where you remove the “spatial” nature of the data aka SVIdata &lt;- as.data.frame(SVI) plot(as.numeric(SVIdata$E_TOTPOP), as.numeric(SVIdata$E_POV),xlab=&quot;Population total&quot;,ylab=&quot;Number living in poverty&quot;) Now go to R graph gallery and look around. If you click on a plot you like then it will show you the code of how to re-create it. Alternatively, there are quite a few tutorials on this which go further than we have: - https://crd150.github.io/lab5.html#sf_vs_sp_spatial_objects - https://mgimond.github.io/Spatial/spatial-autocorrelation-in-r.html - https://rspatial.org/raster/analysis/3-spauto.html 7.7 Lab-5 submission check For this lab, here is the mark breakdown: HTML FILE SUBMISSION - 10 marks RMD CODE SUBMISSION - 10 marks WORKING CODE - 10 marks: Your code works and the output of each code chunk is included in the html file output (e.g. you pressed run-all before you finished) EASY TO READ LAB SCRIPT - 10 marks: You have followed the style guide in section 7.1. CHALLENGE 1 - 20 marks: You thoughtfully completed all the questions, and correctly made your spatial weights matrix CHALLENGE 2 - 35 marks: You thoughtfully completed all the questions, and correctly made your Moran’s analysis for your own state and your own variable. Your write up should include: A description of what the SVI data is and why it’s important (remember the documentation will help) Why you chose the state that you did. Is there something interesting there you want to look at? All the relevant code to make the analysis work for you. What map projection you chose and why. An exploratory analysis of the data and an explanation of why you chose your variable, or your initial thoughts on its spatial distribution (remember the documentation will help) A Moran’s analysis with the results explained clearly in the text. SOMETHING NEW - 5 marks You made a minor change to an existing piece of code (2.5/5) or did 3 data camp chapters. OR You demonstrated the use of a function or concept that was not specifically covered in the handout, lecture, or lab [100 marks total] "]
]

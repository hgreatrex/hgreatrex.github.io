[
["lab-6.html", "Chapter 7 Lab 6 7.1 Lab 6 Set-Up 7.2 Tutorial 1: Spatial weights &amp; Join Counts 7.3 Challenge 1: Join Counts 7.4 Tutorial 2: Social vulnerability and Moran’s I 7.5 Challenge 2: Your own spatial analysis 7.6 Challenge 3 Show me something new 7.7 Lab-5 submission check", " Chapter 7 Lab 6 Objective: Welcome to Lab 6, where we are going to focus on spatial autocorrelaton, join counts and Moran’s I. Specifically you will: Read in shapefiles Have the opportunity to make spatial weights matrices Explore the tmap package Look at join counts data Create Moran’s I plots and interpret them Data-sets: You will be conducting your analysis on some toy data, then examining your own US based county vulnerability data from the CDC. Lab structure: Now you are getting more experienced in R, I will provide a worked example then get you to do something similar on your own data. Although you are of course welcome to copy/paste/run the worked example, YOU SHOULD NOT INCLUDE THE WORKED EXAMPLE IN YOUR FINAL LAB REPORT. 7.1 Lab 6 Set-Up 7.1.1 Create your Lab 6 project file Open a new version of R-studio. Click the file menu, then new project in a new directory. Select your 364 directory, then name the project Lab 6. If you are stuck on this process, see the start of previous labs. You can check this has worked by looking on the file explorer on your computer, then selecting your GEOG364 folder and checking that a Lab 6 folder has appeared with your Lab4.Proj file inside it. 7.1.2 Create your NoteBook file Here you have a choice: Either.. you can create a standard lab script as before: Go to File/New File and create a new R-NOTEBOOK file. Delete the friendly text (everything from line 6 onward) Save your file as GEOG364_Lab6_PSU.ID.Rmd e.g. GEOG364_Lab6_hlg5155.Rmd Follow Section 2.2.2 to modify your YAML code Please make sure that your lab script has a floating table of contents (section 2.2.2, adding the toc_float: TRUE part) Or..OPTIONAL: If you want to explore some new formats, you can try one of the markdown templates stored in R.There are instructions on how to load them on the website here: https://github.com/juba/rmdformats/blob/master/README.md). Again, make sure to save your file as GEOG364_Lab6_PSU.ID.Rmd. Please also add in a floating table of contents to your YAML code. 7.1.3 Style guide A large component of the marks for your labs scripts focuses them being easily readable and easy to follow. Now you have had experience with R, here are some of the things we expect to get full marks: You include a floating table of contents, title and author in the YAML Code You include a “level 1” heading in your text for every lab challenge e.g. # Lab challenge 1 It’s easy to see where your answers are - so you have full sentences, not just one word answers. You have spell-checked your work! The spell check button is between the save button and the knit/preview button. You include blank lines before and after each code chunk, or new paragraph, or bullet point set or heading (put many blank lines in markdown files and R can automatically format them correctly). Any written answers are thoughtful and well considered. As you write your lab, regularly click the Preview or Knit Button and make sure everything looks correct and properly formatted. IF THERE ARE PROBLEMS, TALK TO AN INSTRUCTOR. 7.1.4 Download and run packages Follow the instructions in Section 3.2.2. to download and install the following packages spdep USAboundaries plus any others below that you are missing. Now add a new code chunk in your script. Inside add the following code and run it. library(rgdal) library(sp) library(sf) library(spdep) library(tmap) library(raster) library(USAboundaries) This needs to be at the top of your script because the library commands need to be run every time you open R. Now click the Preview or Knit Button and make sure everything looks correct. Don’t continue until you can make and view your html or nb.html file. If it doesn’t work, ask for help before moving on 7.2 Tutorial 1: Spatial weights &amp; Join Counts First, read the pdf on join counts from canvas - it’s only 3 pages but provides crucial background 7.2.1 Making the grid Join count and autocorrelation statistics are valuable in understanding spatial dependencies among sample units. This first tutorial focuses on using R to calculate join count statistics using a toy dataset. The code below creates a grid, or matrix of data that we can use to test Joins Counts. Here’s a picture of the grid we are going to make. In this case, each polygon is a simple grid cell. Now, I will make the data in R using a new type of data called a matrix (see data camp intro R for more details) row.1 &lt;- rep(1,6) row.2 &lt;- c(0,1,1,1,1,1) row.3 &lt;- c(rep(0,5),1) row.4 &lt;- rep(0,6) row.5 &lt;- c(0,0,0,1,0,0) row.6 &lt;- rep(0,6) matrixA &lt;- matrix(c(row.1, row.2, row.3, row.4, row.5, row.6), nrow=6,ncol=6, byrow=TRUE) matrixA ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 1 1 1 1 1 ## [2,] 0 1 1 1 1 1 ## [3,] 0 0 0 0 0 1 ## [4,] 0 0 0 0 0 0 ## [5,] 0 0 0 1 0 0 ## [6,] 0 0 0 0 0 0 and then we then convert it to spatial data using the raster command. rasterA &lt;- raster(matrixA) plot(rasterA) text(coordinates(rasterA), labels=rasterA[], cex=1.5) 7.2.2 Creating the spatial weights Our raster data will now easily convert to vector data so that we can determine which grid-cells are “nearby”. To find adjacent polygons, we can use package ‘spdep’. First, we covert our raster data to a spatial polygon and it’s coordinates Then we use the poly2nb command to work out the rook’s adjacency Then we plot the data Finally, we calculate the weights matrix. In this case we use a binary (B) criteria, i.e. there is adjacency (1) or there is no adjacency (0). polyA &lt;- rasterToPolygons(rasterA, dissolve=FALSE) spA &lt;- SpatialPolygons(polyA@polygons) nbA.rook &lt;- poly2nb(spA, queen = FALSE) plot(spA, border=&#39;blue&#39;) plot(nbA.rook, coordinates(spA), col=&#39;red&#39;, lwd=2, add=TRUE) weightsA.rook &lt;- nb2listw(nbA.rook, style=&#39;B&#39;) 7.2.3 Conducting a join-counts analysis Now, everything is set for the analyses: We have our raster, which shows the presence/absence of green grid cells, plus the weights information (as object ‘weightsA.rook’). Finally, we apply the Join Count test to evaluate presence or absence or spatial autocorrelation. This is done using a z-test. Slightly differently to the textbook &amp; the lecture notes (which looked for green/white), the R version of this test looks for how likely it is that there are white-to-white joins compared to random chance, but the principle is the same. So we have: H0: The pattern exhibits complete spatial randomness or is dispersed. The number of Observed “same color” joins (OWhite-White) is no higher than what you would expect from an Independent Random Process: OWW = EWW H1: The pattern is not random. The number of “same color” joins is unusually high (AKA the pattern is clustered): OWW &gt; EWW Here is the code for the Z-test jc_testA &lt;- joincount.test(fx= as.factor(polyA$layer), #data listw= weightsA.rook, #weights alternative=&quot;greater&quot;) Another thing to note is that you can select which different tailed alternative hypothesis: (default) ‘greater’: the alternative hypothesis that the number of like joins is more than expected by random chance. (THIS IS ONLY TRUE IF THE THING YOU ARE TESTING IS WW or GG joins. The opposite would be the case if you tested GW joins, think through why this is the case) ‘less’: the alternative hypothesis that the number of like joins is fewer than expected by random chance (indicating higher levels of dispersion). (IF THE THING YOU ARE TESTING IS WW or GG joins. The opposite would be the case if you tested GW joins.) ‘two.sided’: the alternative hypothesis that the number of like joins is simply different to the number you would expect from random chance. This is unusual to use as we are normally looking for either clustering or dispersion. joincount.test actually does two tests, so we use double square brackets [[ ]] to show the first and second test separately: # White-to-white Join counts jc_testA[[1]] ## ## Join count test under nonfree sampling ## ## data: as.factor(polyA$layer) ## weights: weightsA.rook ## ## Std. deviate for 0 = 4.1544, p-value = 1.631e-05 ## alternative hypothesis: greater ## sample estimates: ## Same colour statistic Expectation Variance ## 33.000000 24.095238 4.594478 # Green-to-green Join counts jc_testA[[2]] ## ## Join count test under nonfree sampling ## ## data: as.factor(polyA$layer) ## weights: weightsA.rook ## ## Std. deviate for 1 = 4.06, p-value = 2.453e-05 ## alternative hypothesis: greater ## sample estimates: ## Same colour statistic Expectation Variance ## 15.000000 7.428571 3.477764 We can see that there are many more white-white joins / green-green joins than you might expect. In fact, looking at the low p-value, we can see that it is very unusual to see so many same-color joins, so in this case, most people can can safely reject the null hypothesis. 7.3 Challenge 1: Join Counts 7.3.1 Challenge 1a Have you read the pdf on join counts from canvas? It’s only 3 pages but provides crucial background Start a new section called Challenge 1, and a new level 2 sub heading called Challenge 1a. Underneath, summarise what spatial autocorrelation is, referencing: Tobler’s law, What is a global autocorrelation measure What is a local autocorrelation measure 7.3.2 Challenge 1b Make a new sub-heading called challenge 1b. Copy/use the code in the tutorial above to conduct your own analysis on your own new matrix that you create called matrixB. Try to make a grid that has is likely to have negative spatial autocorrelation, but still has a little bit of randomness. Use a queen’s weights matrix in your example You should conduct a hypothesis test at a significance level of p-value:5% to see if the data is unusually clustered and interpret what this means. As part of your answer, you should also explain what the rep function which is included in the tutorial. To get full marks you need neat code, comments &amp; good variable names (we’re doing grid B not grid A!). 7.4 Tutorial 2: Social vulnerability and Moran’s I This next tutorial applies what we have learned to a real life scenario, that of vulnerability modelling in the USA. The Centers for Disease Control and Prevention Social Vulnerability Index (SVI) was created to help public health officials and emergency response planners identify and map the communities that will most likely need support before, during, and after a hazardous event. SVI indicates the relative vulnerability of every U.S. Census tract. Census tracts are subdivisions of counties for which the Census collects statistical data. SVI ranks the tracts on 15 social factors, including unemployment, minority status, and disability, and further groups them into four related themes. Thus, each tract receives a ranking for each Census variable and for each of the four themes, as well as an overall ranking. I am going to use this data for a specific state to look at the spatial distribution of some of these variables. 7.4.1 Download the data First I will find and download the data. I found the SVI data at https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html The documentation to understand what the column headings of the data can be found at: https://www.atsdr.cdc.gov/placeandhealth/svi/documentation/pdf/SVI2018Documentation-H.pdf I then downloaded county level data for Florida for 2018 as an ESRI shapefile. I saved this to my lab 6 folder, then unzipped it. 7.4.2 Read the data into R The readOGR command will read a shapefile into R. You should split your command into the “dsn”, the folder containing the data and the “layer”, the name of the file with NO extension In our case, we are using the Lab 6 r-project to set our folder (the “.” below), then we just enter the name of the sub-folder and the file. If you can’t make it work, then running file.choose() in the console is a good way to get the correct wording. # This reads in the shapefile containing the SVI Data and saves it to an sp variable called SVI SVI &lt;- readOGR(dsn=&quot;./Florida_COUNTY/&quot;, layer=&quot;SVI2018_FLORIDA_county&quot;) I will also use the us_states command from the USAboundaries package to download the border of Florida. # This downloads the US state border data for my state, it&#39;s from the USAboundaries package State.border &lt;- us_states(states = &quot;Florida&quot;) # convert to sp State.border &lt;- as(State.border,Class=&quot;Spatial&quot;) 7.4.3 Change the projection Now let’s check the projection of both and make sure they are the same. projection(SVI) ## [1] &quot;+proj=longlat +datum=NAD83 +no_defs&quot; projection(State.border) ## [1] &quot;+proj=longlat +datum=WGS84 +no_defs&quot; Although they are both equal, I’m now going to change them both to a UTM projection as it’s more robust. I found the code by selecting Florida UTM here: https://mangomap.com/robertyoung/maps/69585/what-utm-zone-am-i-in-# Then checking the epsg code here: http://epsg.io/ # I&#39;m using the suppressWarnigngs command to get rid of those CRS messages/warnings SVI &lt;- suppressWarnings(spTransform(SVI,CRS(&quot;+init=epsg:3747&quot;))) State.border &lt;- suppressWarnings(spTransform(State.border,CRS(&quot;+init=epsg:3747&quot;))) and check it worked: projection(SVI) ## [1] &quot;+proj=utm +zone=17 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; projection(State.border) ## [1] &quot;+proj=utm +zone=17 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; 7.4.4 Explore the data Now I can explore the dataset. I have already looked at the documentation and seen that there are many columns I can look at. I can see these by showing the column names here: names(SVI) ## [1] &quot;ST&quot; &quot;STATE&quot; &quot;ST_ABBR&quot; &quot;COUNTY&quot; &quot;FIPS&quot; &quot;LOCATION&quot; ## [7] &quot;AREA_SQMI&quot; &quot;E_TOTPOP&quot; &quot;M_TOTPOP&quot; &quot;E_HU&quot; &quot;M_HU&quot; &quot;E_HH&quot; ## [13] &quot;M_HH&quot; &quot;E_POV&quot; &quot;M_POV&quot; &quot;E_UNEMP&quot; &quot;M_UNEMP&quot; &quot;E_PCI&quot; ## [19] &quot;M_PCI&quot; &quot;E_NOHSDP&quot; &quot;M_NOHSDP&quot; &quot;E_AGE65&quot; &quot;M_AGE65&quot; &quot;E_AGE17&quot; ## [25] &quot;M_AGE17&quot; &quot;E_DISABL&quot; &quot;M_DISABL&quot; &quot;E_SNGPNT&quot; &quot;M_SNGPNT&quot; &quot;E_MINRTY&quot; ## [31] &quot;M_MINRTY&quot; &quot;E_LIMENG&quot; &quot;M_LIMENG&quot; &quot;E_MUNIT&quot; &quot;M_MUNIT&quot; &quot;E_MOBILE&quot; ## [37] &quot;M_MOBILE&quot; &quot;E_CROWD&quot; &quot;M_CROWD&quot; &quot;E_NOVEH&quot; &quot;M_NOVEH&quot; &quot;E_GROUPQ&quot; ## [43] &quot;M_GROUPQ&quot; &quot;EP_POV&quot; &quot;MP_POV&quot; &quot;EP_UNEMP&quot; &quot;MP_UNEMP&quot; &quot;EP_PCI&quot; ## [49] &quot;MP_PCI&quot; &quot;EP_NOHSDP&quot; &quot;MP_NOHSDP&quot; &quot;EP_AGE65&quot; &quot;MP_AGE65&quot; &quot;EP_AGE17&quot; ## [55] &quot;MP_AGE17&quot; &quot;EP_DISABL&quot; &quot;MP_DISABL&quot; &quot;EP_SNGPNT&quot; &quot;MP_SNGPNT&quot; &quot;EP_MINRTY&quot; ## [61] &quot;MP_MINRTY&quot; &quot;EP_LIMENG&quot; &quot;MP_LIMENG&quot; &quot;EP_MUNIT&quot; &quot;MP_MUNIT&quot; &quot;EP_MOBILE&quot; ## [67] &quot;MP_MOBILE&quot; &quot;EP_CROWD&quot; &quot;MP_CROWD&quot; &quot;EP_NOVEH&quot; &quot;MP_NOVEH&quot; &quot;EP_GROUPQ&quot; ## [73] &quot;MP_GROUPQ&quot; &quot;EPL_POV&quot; &quot;EPL_UNEMP&quot; &quot;EPL_PCI&quot; &quot;EPL_NOHSDP&quot; &quot;SPL_THEME1&quot; ## [79] &quot;RPL_THEME1&quot; &quot;EPL_AGE65&quot; &quot;EPL_AGE17&quot; &quot;EPL_DISABL&quot; &quot;EPL_SNGPNT&quot; &quot;SPL_THEME2&quot; ## [85] &quot;RPL_THEME2&quot; &quot;EPL_MINRTY&quot; &quot;EPL_LIMENG&quot; &quot;SPL_THEME3&quot; &quot;RPL_THEME3&quot; &quot;EPL_MUNIT&quot; ## [91] &quot;EPL_MOBILE&quot; &quot;EPL_CROWD&quot; &quot;EPL_NOVEH&quot; &quot;EPL_GROUPQ&quot; &quot;SPL_THEME4&quot; &quot;RPL_THEME4&quot; ## [97] &quot;SPL_THEMES&quot; &quot;RPL_THEMES&quot; &quot;F_POV&quot; &quot;F_UNEMP&quot; &quot;F_PCI&quot; &quot;F_NOHSDP&quot; ## [103] &quot;F_THEME1&quot; &quot;F_AGE65&quot; &quot;F_AGE17&quot; &quot;F_DISABL&quot; &quot;F_SNGPNT&quot; &quot;F_THEME2&quot; ## [109] &quot;F_MINRTY&quot; &quot;F_LIMENG&quot; &quot;F_THEME3&quot; &quot;F_MUNIT&quot; &quot;F_MOBILE&quot; &quot;F_CROWD&quot; ## [115] &quot;F_NOVEH&quot; &quot;F_GROUPQ&quot; &quot;F_THEME4&quot; &quot;F_TOTAL&quot; &quot;E_UNINSUR&quot; &quot;M_UNINSUR&quot; ## [121] &quot;EP_UNINSUR&quot; &quot;MP_UNINSUR&quot; &quot;E_DAYPOP&quot; &quot;Shape_STAr&quot; &quot;Shape_STLe&quot; There are lots more non-spatial plots I can make, but for now I will move onto maps. Today I am going to use tmap. As you can see, a plot is built up in layers. Each tm_shape line identifies some data you want to plot, then the indented lines afterwards plot it. I have chosen to plot one of the many variables that are available to me:E_TOTPOP. From the documentation I found this represents the population total in each county. tm_shape(SVI) + tm_polygons(col=&quot;E_TOTPOP&quot;, style=&quot;quantile&quot;, border.col = &quot;black&quot;, palette=&quot;Spectral&quot;) + tm_legend(outside = TRUE) + tm_shape(State.border) + tm_borders() + tm_layout(main.title = &quot;Population&quot;, main.title.size = 0.95) ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output ## Warning: Number of levels of the variable &quot;E_TOTPOP&quot; is 67, which is larger than ## max.categories (which is 30), so levels are combined. Set tmap_options(max.categories = ## 67) in the layer function to show all levels. Now we have a new problem (this is real life data!). I expected the data to be numeric - e.g. the average annual income in dollars, but R has not understood this. This is why the colorbar looks weird and why R is giving me the warning message about “too many breaks”, when I made the plot above. If I print SVI$E_TOTPOP onto the screen then I noticed that each one has quote marks around it, so R thinks its text. print(SVI$E_TOTPOP) ## [1] &quot;235503&quot; &quot;80578&quot; &quot;120999&quot; &quot;412144&quot; &quot;107139&quot; &quot;455086&quot; &quot;170442&quot; &quot;157581&quot; ## [9] &quot;576808&quot; &quot;200737&quot; &quot;31877&quot; &quot;150984&quot; &quot;176954&quot; &quot;207291&quot; &quot;76325&quot; &quot;311522&quot; ## [17] &quot;263148&quot; &quot;373853&quot; &quot;957875&quot; &quot;527634&quot; &quot;182696&quot; &quot;14105&quot; &quot;363922&quot; &quot;16055&quot; ## [25] &quot;65858&quot; &quot;718679&quot; &quot;288102&quot; &quot;143087&quot; &quot;335362&quot; &quot;27785&quot; &quot;510593&quot; &quot;182482&quot; ## [33] &quot;1446277&quot; &quot;17615&quot; &quot;348371&quot; &quot;305591&quot; &quot;1909151&quot; &quot;39961&quot; &quot;11736&quot; &quot;1321194&quot; ## [41] &quot;1378883&quot; &quot;924229&quot; &quot;14444&quot; &quot;338619&quot; &quot;69105&quot; &quot;8365&quot; &quot;16437&quot; &quot;102101&quot; ## [49] &quot;15239&quot; &quot;24566&quot; &quot;26979&quot; &quot;22098&quot; &quot;668671&quot; &quot;13363&quot; &quot;19430&quot; &quot;2715516&quot; ## [57] &quot;43924&quot; &quot;40572&quot; &quot;18474&quot; &quot;72766&quot; &quot;48472&quot; &quot;36399&quot; &quot;27228&quot; &quot;46017&quot; ## [65] &quot;8744&quot; &quot;40127&quot; &quot;14269&quot; Let’s use the as.numeric command to turn that column back into numbers SVI$E_TOTPOP &lt;- as.numeric(SVI$E_TOTPOP) print(SVI$E_TOTPOP) ## [1] 235503 80578 120999 412144 107139 455086 170442 157581 576808 200737 ## [11] 31877 150984 176954 207291 76325 311522 263148 373853 957875 527634 ## [21] 182696 14105 363922 16055 65858 718679 288102 143087 335362 27785 ## [31] 510593 182482 1446277 17615 348371 305591 1909151 39961 11736 1321194 ## [41] 1378883 924229 14444 338619 69105 8365 16437 102101 15239 24566 ## [51] 26979 22098 668671 13363 19430 2715516 43924 40572 18474 72766 ## [61] 48472 36399 27228 46017 8744 40127 14269 and now when we plot.. tm_shape(SVI) + tm_polygons(col=&quot;E_TOTPOP&quot;, style=&quot;pretty&quot;, border.col = &quot;black&quot;, palette=&quot;Spectral&quot;) + tm_legend(outside = TRUE) + tm_shape(State.border) + tm_borders() + tm_layout(main.title = &quot;Total population&quot;, main.title.size = 0.95) ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output That looks better! So it seems that my population is clustered in the south and mid-way up state. This makes sense given that the blue area is the Miami-Dade district. To better understand my data, there are two other ways I might want to use tmap: Firstly, I might want to compare two maps at once. I can do this by creating two separate maps and instead of plotting them, simply saving them as Map1 and Map2, then plotting them together using the tmap_arrange command. I might also want to zoom in on these in the way we earlier made leaflet plot. In this case, we simply switch map view from “plot” to “view”. I also added a leaflet basemap (code from here: http://leaflet-extras.github.io/leaflet-providers/preview/) For other color palettes, see here: https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing # create map 1 Map1 &lt;- tm_basemap(&quot;OpenStreetMap.Mapnik&quot;) + # Set the watercolor basemap tm_shape(SVI) + # plot the SVI data tm_polygons(col=&quot;E_TOTPOP&quot;, # name of the column I am plotting style=&quot;pretty&quot;, # do i want color breaks, or continous color bar? border.col = NULL, # no outlines palette=&quot;Spectral&quot;, #color palette title=&quot;Population Density&quot;, #plot title alpha = .5) + #slightly transparent tm_shape(State.border) + # Plot the State.border data tm_borders() #plot it as borders/lines # remember to convert any new numeric variables into numbers SVI$E_PCI &lt;- as.numeric(SVI$E_PCI) # create map 2 Map2 &lt;- tm_basemap(&quot;Stamen.Watercolor&quot;) + tm_shape(SVI) + tm_polygons(col=&quot;E_PCI&quot;,style=&quot;pretty&quot;, border.col = NULL, palette=&quot;RdBu&quot;, title=&quot;Average income&quot;, alpha = .7) + tm_shape(State.border) + tm_borders() # plot them both tmap_arrange(Map1,Map2) ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output 7.4.5 Creating a spatial weights matrix For the rest of this lab I am going to focus on population density. First, I look to see if the population density data looks like it clusters or is dispersed. I can also look at other data to understand why this might be the case (e.g. is there some underlying reason why our data looks this way). From my perspective and the color scale, it does look like there is clustering of high population areas. As described in the first part of the lab, before we can formally model the spatial dependency shown in the above map, we must first cover how neighborhoods are spatially connected to one another. That is, what does “near” mean when we say “near things are more related than distant things”? For each census tract, we need to define What counts as its neighbor (connectivity) How much does each neighbor matter? (spatial weights) To do this we again calculate the spatial weights matrix: # calculate the spatial weights matrix spatial.matrix.rook &lt;-poly2nb(SVI, queen=F) plot(SVI, border=&#39;blue&#39;) plot(spatial.matrix.rook, coordinates(SVI), col=&#39;black&#39;, lwd=2, add=TRUE) 7.4.5.1 Moran’s I scatterplot We can then assign weights to each one based on whether it is a neighbour or not. Again, I am just using rook’s contingency, a binary classification of 1 if a county is a neighbour and 0 if not. # calculate the spatial weights weights.rook &lt;- nb2listw(spatial.matrix.rook, style=&#39;B&#39;) and now I will calculate the Moran’s scatterplot ## and calculate the moran&#39;s plot moran.plot(SVI$E_TOTPOP, weights.rook, xlab = &quot;Population&quot;, ylab = &quot;Neighbors Population&quot;, labels=SVI$COUNTY) Here we can see that high population counties tend to be clustered together, but that there is not a perfect relationship. Miami Dade in particular is skewing the relationship. The “Moran’s I” global statistic is essentially a summary of this plot. We look at the correlation coeffient between the two variables. Figure 7.1: plot from: https://mgimond.github.io/Spatial/spatial-autocorrelation.html The easiest way to see this in R is to use the Moran.test command, which also assesses the significance of your result: moran.test(SVI$E_TOTPOP, weights.rook) ## ## Moran I test under randomisation ## ## data: SVI$E_TOTPOP ## weights: weights.rook ## ## Moran I statistic standard deviate = 4.205, p-value = 1.305e-05 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.297170683 -0.015151515 0.005516543 We find that the Moran’s I is positive (0.30) and statistically significant (p-value &lt; 0.05). Remember from lecture that the Moran’s I is simply a correlation, and correlations go from -1 to 1. A 0.30 correlation is relatively high, indicating some clustering, but not a very strong signal. Moreover, we find that this correlation is statistically significant (p-value basically at 0). 7.5 Challenge 2: Your own spatial analysis Make a new heading challenge 2. In here, I would like you to present a spatial autocorrelation analysis of your own. This will be easier in week 2 after the lecture on Moran’s I. Your job is to repeat this tutorial in your own .rmd file but for a DIFFERENT STATE and for a DIFFERENT VARIABLE. (not Florida and not Population density) Your write up should include: A description of what the SVI data is and why it’s important (remember the documentation will help) Why you chose the state that you did. Is there something interesting there you want to look at? All the relevant code to make the analysis work for you. What map projection you chose and why. An exploratory analysis of the data and an explanation of why you chose your variable, or your initial thoughts on its spatial distribution (remember the documentation will help) A Moran’s analysis with the results explained clearly in the text. For full grades, the code will also show some originality e.g. making the plots feel like “yours” (choosing your own color palettes, map backgrounds etc), including markdown elements such as references or equations as relevant, including section sub-headings and thoughtful analysis. 7.6 Challenge 3 Show me something new Please describe what you are trying to achieve. For 2.5/5 marks 3 NEW chapters of a data camp course and include completion screen-shots in your lab script You can make some minor changes, for example, you could use this equation editor - https://www.codecogs.com/latex/eqneditor.php - and this tutorial - https://www.math.mcgill.ca/yyang/regression/RMarkdown/example.html, you could include a mathematical equation of your choice in your markdown text. You could make some minor changes to some of the code, for example adding some new functionality to tmap. OR for 5/5 you can continue to do the classic “something new”, where you need to demonstrates the use of a function or package that was not specifically covered in the handout, lecture, or lab. Remember you actually have to do something new, not repeat what you did in previous weeks My strong recommendation is to do something cool with the data you have. For example, you have loads of data you can make scatterplots with. The easiest way to do this is to make a new table where you remove the “spatial” nature of the data aka SVIdata &lt;- as.data.frame(SVI) plot(as.numeric(SVIdata$E_TOTPOP), as.numeric(SVIdata$E_POV),xlab=&quot;Population total&quot;,ylab=&quot;Number living in poverty&quot;) Now go to R graph gallery and look around. If you click on a plot you like then it will show you the code of how to re-create it. Alternatively, there are quite a few tutorials on this which go further than we have: - https://crd150.github.io/lab5.html#sf_vs_sp_spatial_objects - https://mgimond.github.io/Spatial/spatial-autocorrelation-in-r.html - https://rspatial.org/raster/analysis/3-spauto.html 7.7 Lab-5 submission check For this lab, here is the mark breakdown: HTML FILE SUBMISSION - 10 marks RMD CODE SUBMISSION - 10 marks WORKING CODE - 10 marks: Your code works and the output of each code chunk is included in the html file output (e.g. you pressed run-all before you finished) EASY TO READ LAB SCRIPT - 10 marks: You have followed the style guide in section 7.1. CHALLENGE 1 - 20 marks: You thoughtfully completed all the questions, and correctly made your spatial weights matrix CHALLENGE 2 - 35 marks: You thoughtfully completed all the questions, and correctly made your Moran’s analysis for your own state and your own variable. Your write up should include: A description of what the SVI data is and why it’s important (remember the documentation will help) Why you chose the state that you did. Is there something interesting there you want to look at? All the relevant code to make the analysis work for you. What map projection you chose and why. An exploratory analysis of the data and an explanation of why you chose your variable, or your initial thoughts on its spatial distribution (remember the documentation will help) A Moran’s analysis with the results explained clearly in the text. SOMETHING NEW - 5 marks You made a minor change to an existing piece of code (2.5/5) or did 3 data camp chapters. OR You demonstrated the use of a function or concept that was not specifically covered in the handout, lecture, or lab [100 marks total] "]
]
